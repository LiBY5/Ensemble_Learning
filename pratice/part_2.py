# Stackingé›†æˆä¼˜åŒ– - ç¬¬ä¸‰æ­¥ï¼šå¼•å…¥æ–°æ¨¡å‹ä¸ä¸¤å±‚Stacking
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge, Lasso
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.model_selection import KFold, RandomizedSearchCV
from scipy.stats import randint, uniform, loguniform
from sklearn.metrics import mean_squared_error
import warnings

warnings.filterwarnings('ignore')

print("=" * 60)
print("Stackingé›†æˆä¼˜åŒ– - ç¬¬ä¸‰æ­¥ï¼šå¼•å…¥æ–°æ¨¡å‹ä¸ä¸¤å±‚Stacking")
print("=" * 60)

# 1. åŠ è½½æ•°æ®
X_train = pd.read_csv('X_train_processed.csv')
y_train = pd.read_csv('y_train_log.csv').values.ravel()
X_test = pd.read_csv('X_test_processed.csv')
feature_names = X_train.columns.tolist()
X_train = X_train.values
X_test = X_test.values

print("æ•°æ®åŠ è½½å®Œæˆï¼Œå¼€å§‹ç¬¬ä¸‰æ­¥ä¼˜åŒ–ã€‚")

# 2. å¯¹æœªå……åˆ†è°ƒä¼˜çš„åŸºæ¨¡å‹è¿›è¡Œè°ƒä¼˜ (RandomForest, XGBoost, å¹¶å¼•å…¥CatBoost)
print("\n--- æ­¥éª¤1: å¯¹å…³é”®åŸºæ¨¡å‹è¿›è¡Œè°ƒä¼˜ ---")


# ç”±äºè°ƒä¼˜è€—æ—¶ï¼Œæˆ‘ä»¬è¿™é‡Œè¿›è¡Œç®€åŒ–ç‰ˆçš„éšæœºæœç´¢ï¼Œå¹¶é™åˆ¶è¿­ä»£æ¬¡æ•°
def quick_tune(model, param_dist, X, y, model_name, n_iter=10):
    print(f"  æ­£åœ¨è°ƒä¼˜ {model_name}...")
    search = RandomizedSearchCV(
        model, param_dist, n_iter=n_iter, cv=3,
        scoring='neg_mean_squared_error', random_state=42, n_jobs=-1
    )
    search.fit(X, y)
    best_rmse = np.sqrt(-search.best_score_)
    print(f"    æœ€ä½³å‚æ•°: {search.best_params_}")
    print(f"    æœ€ä½³RMSE: {best_rmse:.5f}")
    return search.best_estimator_


# 2.1 è°ƒä¼˜ RandomForest
rf = RandomForestRegressor(random_state=42, n_jobs=-1)
rf_param_dist = {
    'n_estimators': randint(100, 300),
    'max_depth': randint(5, 15),
    'min_samples_split': randint(2, 10),
    'max_features': uniform(0.1, 0.5)  # é™åˆ¶ç‰¹å¾æ¯”ä¾‹ï¼Œå¢åŠ éšæœºæ€§
}
best_rf = quick_tune(rf, rf_param_dist, X_train, y_train, "RandomForest")

# 2.2 è°ƒä¼˜ XGBoost
xgb = XGBRegressor(random_state=42, n_jobs=-1, verbosity=0)
xgb_param_dist = {
    'n_estimators': randint(100, 300),
    'max_depth': randint(3, 8),
    'learning_rate': loguniform(0.01, 0.3),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4)
}
best_xgb = quick_tune(xgb, xgb_param_dist, X_train, y_train, "XGBoost")

# 2.3 å¼•å…¥å¹¶è°ƒä¼˜ CatBoost
print("  æ­£åœ¨è°ƒä¼˜ CatBoost...")
# CatBoostè°ƒä¼˜ç¨æ…¢ï¼Œæˆ‘ä»¬ä½¿ç”¨è¾ƒå°‘å‚æ•°
cb = CatBoostRegressor(random_state=42, verbose=0, thread_count=-1)
cb_param_dist = {
    'iterations': randint(100, 300),
    'depth': randint(4, 8),
    'learning_rate': loguniform(0.01, 0.3),
    'l2_leaf_reg': randint(1, 10)
}
best_cb = quick_tune(cb, cb_param_dist, X_train, y_train, "CatBoost")

# 2.4 ä½¿ç”¨ä¹‹å‰è°ƒä¼˜å¥½çš„Lasso, Ridge, LightGBM
best_lasso = Lasso(alpha=0.000534, max_iter=50000, random_state=42)
best_ridge = Ridge(alpha=10.0, random_state=42)
best_lgb = LGBMRegressor(
    n_estimators=216, learning_rate=0.101, max_depth=3,
    num_leaves=37, subsample=0.98, colsample_bytree=0.94,
    random_state=42, verbose=-1
)

# 3. æ„å»ºå¼ºå¤§çš„ç¬¬ä¸€å±‚åŸºæ¨¡å‹åˆ—è¡¨
base_models = [
    ('lasso', best_lasso),
    ('ridge', best_ridge),
    ('lightgbm', best_lgb),
    ('random_forest', best_rf),
    ('xgboost', best_xgb),
    ('catboost', best_cb)  # æ–°å¢
]
print(f"\nç¬¬ä¸€å±‚å°†ä½¿ç”¨ {len(base_models)} ä¸ªè°ƒä¼˜åçš„åŸºæ¨¡å‹ã€‚")

# 4. å®ç°ä¸¤å±‚Stacking
print("\n--- æ­¥éª¤2: è®­ç»ƒä¸¤å±‚Stackingé›†æˆ ---")

# 4.1 ç¬¬ä¸€å±‚ï¼šç”ŸæˆOOFé¢„æµ‹å’Œæµ‹è¯•é›†é¢„æµ‹
n_folds = 7
kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)

train_meta_features = np.zeros((X_train.shape[0], len(base_models)))
test_meta_features = np.zeros((X_test.shape[0], len(base_models)))

print(f"ä½¿ç”¨ {n_folds} æŠ˜CVç”Ÿæˆç¬¬ä¸€å±‚é¢„æµ‹ (å…ƒç‰¹å¾)...")
for i, (name, model) in enumerate(base_models):
    print(f"  åŸºæ¨¡å‹: {name:15}", end="")
    test_fold_preds = []
    for train_idx, val_idx in kf.split(X_train, y_train):
        X_tr, X_val = X_train[train_idx], X_train[val_idx]
        y_tr = y_train[train_idx]
        model_clone = model.__class__(**model.get_params()) if hasattr(model, 'get_params') else model
        model_clone.fit(X_tr, y_tr)
        train_meta_features[val_idx, i] = model_clone.predict(X_val)
        test_fold_preds.append(model_clone.predict(X_test))
    # æµ‹è¯•é›†é¢„æµ‹å–å„æŠ˜å¹³å‡
    test_meta_features[:, i] = np.mean(test_fold_preds, axis=0)
    model_rmse = np.sqrt(mean_squared_error(y_train, train_meta_features[:, i]))
    print(f"  OOF RMSE: {model_rmse:.5f}")

print(f"ç¬¬ä¸€å±‚å…ƒç‰¹å¾å½¢çŠ¶: {train_meta_features.shape}")

# 4.2 é€‰æ‹©æœ€é‡è¦çš„åŸå§‹ç‰¹å¾ï¼Œä¸ç¬¬ä¸€å±‚é¢„æµ‹æ‹¼æ¥
print("\n--- æ­¥éª¤3: é€‰æ‹©é‡è¦åŸå§‹ç‰¹å¾ï¼Œæ„å»ºç¬¬äºŒå±‚ç‰¹å¾ ---")
# ä½¿ç”¨Lassoçš„ç‰¹å¾é€‰æ‹©èƒ½åŠ›ï¼Œæ‰¾å‡ºæœ€é‡è¦çš„åŸå§‹ç‰¹å¾
selector = Lasso(alpha=0.0005, max_iter=10000, random_state=42)
selector.fit(X_train, y_train)
# è·å–ç³»æ•°ç»å¯¹å€¼å¤§äºé˜ˆå€¼çš„ç‰¹å¾ç´¢å¼•
important_feature_idx = np.where(np.abs(selector.coef_) > 1e-4)[0]
print(f"ä» {X_train.shape[1]} ä¸ªåŸå§‹ç‰¹å¾ä¸­é€‰æ‹©äº† {len(important_feature_idx)} ä¸ªé‡è¦ç‰¹å¾ã€‚")
if len(important_feature_idx) > 20:  # å¦‚æœå¤ªå¤šï¼Œåªå–å‰20ä¸ª
    coef_abs = np.abs(selector.coef_[important_feature_idx])
    top_idx = np.argsort(coef_abs)[-20:]
    important_feature_idx = important_feature_idx[top_idx]
    print(f"ä¿ç•™æœ€é‡è¦çš„ {len(important_feature_idx)} ä¸ªç‰¹å¾ç”¨äºç¬¬äºŒå±‚ã€‚")

# æ„å»ºç¬¬äºŒå±‚ç‰¹å¾ = [ç¬¬ä¸€å±‚é¢„æµ‹, é‡è¦åŸå§‹ç‰¹å¾]
X_train_layer2 = np.hstack([train_meta_features, X_train[:, important_feature_idx]])
X_test_layer2 = np.hstack([test_meta_features, X_test[:, important_feature_idx]])
print(f"ç¬¬äºŒå±‚ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_train_layer2.shape}")

# 4.3 ç¬¬äºŒå±‚ï¼šè®­ç»ƒå…ƒæ¨¡å‹
print("\n--- æ­¥éª¤4: è®­ç»ƒç¬¬äºŒå±‚å…ƒæ¨¡å‹ ---")
# ä½¿ç”¨ç®€å•çš„Ridgeå›å½’ï¼Œé¿å…è¿‡æ‹Ÿåˆ
meta_model = Ridge(alpha=1.0, random_state=42)

# è¯„ä¼°ä¸¤å±‚Stackingæ€§èƒ½
print("è¯„ä¼°ä¸¤å±‚Stackingæ€§èƒ½ (5æŠ˜CV)...")
cv_scores = []
for train_idx, val_idx in kf.split(X_train_layer2, y_train):
    if len(train_idx) < n_folds:  # ç¡®ä¿è®­ç»ƒé›†è¶³å¤Ÿå¤§
        continue
    X_tr2, X_val2 = X_train_layer2[train_idx], X_train_layer2[val_idx]
    y_tr, y_val = y_train[train_idx], y_train[val_idx]
    meta_model.fit(X_tr2, y_tr)
    val_pred = meta_model.predict(X_val2)
    score = np.sqrt(mean_squared_error(y_val, val_pred))
    cv_scores.append(score)

mean_rmse = np.mean(cv_scores)
std_rmse = np.std(cv_scores)
print(f"ä¸¤å±‚Stacking å¹³å‡RMSE: {mean_rmse:.5f} (Â±{std_rmse:.5f})")

# 5. æ€§èƒ½æœ€ç»ˆå¯¹æ¯”ä¸å†³ç­–
print("\n" + "=" * 60)
print("æœ€ç»ˆæ€§èƒ½å¯¹æ¯”")
print("=" * 60)
comparison = {
    'Best Single Model (Lasso)': 0.10417,
    'Step1 Optimized Stacking': 0.11113,
    'Step3 Two-Layer Stacking': mean_rmse
}

print(f"{'Model':<35} | {'RMSE':<10} | {'Improvement vs Lasso':<20}")
print("-" * 70)
for model, rmse in comparison.items():
    if model != 'Best Single Model (Lasso)':
        impr = (0.10417 - rmse) / 0.10417 * 100
        print(f"{model:<35} | {rmse:<10.5f} | {impr:>+6.2f}%")
    else:
        print(f"{model:<35} | {rmse:<10.5f} | {'(åŸºå‡†)':>20}")

# 6. è®­ç»ƒæœ€ç»ˆæ¨¡å‹å¹¶ç”Ÿæˆé¢„æµ‹
if mean_rmse < 0.11113:  # å¦‚æœä¼˜äºç¬¬ä¸€æ­¥ä¼˜åŒ–
    print(f"\nâœ… ä¸¤å±‚Stackingæ€§èƒ½ä¼˜äºç¬¬ä¸€æ­¥ä¼˜åŒ–ã€‚")
    if mean_rmse < 0.10417:
        print(f"ğŸ‰ çªç ´ï¼ä¸¤å±‚Stacking ({mean_rmse:.5f}) é¦–æ¬¡è¶…è¶Šæœ€ä½³å•æ¨¡å‹Lasso (0.10417)!")
        improvement = (0.10417 - mean_rmse) / 0.10417 * 100
        print(f"   ç›¸å¯¹æå‡: {improvement:.2f}%")
    else:
        print(f"âš ï¸  è™½æœªè¶…è¶Šå•æ¨¡å‹ï¼Œä½†ä¼˜äºä¹‹å‰æ‰€æœ‰Stackingå˜ä½“ã€‚")

    print("\nè®­ç»ƒæœ€ç»ˆä¸¤å±‚Stackingæ¨¡å‹ç”¨äºæµ‹è¯•é›†é¢„æµ‹...")
    # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒç¬¬ä¸€å±‚ï¼ˆç®€åŒ–ï¼Œå®é™…åº”ä¿å­˜å„æŠ˜æ¨¡å‹ï¼‰
    for i, (name, model) in enumerate(base_models):
        model.fit(X_train, y_train)  # åœ¨å…¨é‡æ•°æ®ä¸Šè®­ç»ƒ
    # ç”Ÿæˆæœ€ç»ˆæµ‹è¯•é›†å…ƒç‰¹å¾
    test_meta_final = np.column_stack([model.predict(X_test) for _, model in base_models])
    X_test_final = np.hstack([test_meta_final, X_test[:, important_feature_idx]])
    # åœ¨å…¨é‡ç¬¬äºŒå±‚ç‰¹å¾ä¸Šè®­ç»ƒå…ƒæ¨¡å‹
    meta_model_final = Ridge(alpha=1.0, random_state=42)
    meta_model_final.fit(X_train_layer2, y_train)
    # é¢„æµ‹
    test_pred_log = meta_model_final.predict(X_test_final)

    # è½¬æ¢å›åŸå§‹æˆ¿ä»·å¹¶ä¿å­˜
    test_pred_price = np.expm1(test_pred_log)
    test_ids = pd.read_csv('test_ids.csv')['Id']
    final_submission = pd.DataFrame({
        'Id': test_ids,
        'SalePrice': test_pred_price
    })
    submission_path = 'Prediction.csv'
    final_submission.to_csv(submission_path, index=False)
    print(f"æœ€ç»ˆæäº¤æ–‡ä»¶å·²ä¿å­˜: {submission_path}")
    print("æ–‡ä»¶é¢„è§ˆ:")
    print(final_submission.head())
else:
    print(f"\nâŒ ä¸¤å±‚Stackingæœªå¸¦æ¥æå‡ã€‚éœ€è¦é‡æ–°è¯„ä¼°ä¼˜åŒ–ç­–ç•¥ã€‚")

# 7. åŸºæ¨¡å‹è´¡çŒ®åº¦åˆ†æ
print("\n--- åŸºæ¨¡å‹è´¡çŒ®åº¦åˆ†æ ---")
# é€šè¿‡å…ƒæ¨¡å‹çš„ç³»æ•°ç»å¯¹å€¼ï¼Œåˆ†ææ¯ä¸ªåŸºæ¨¡å‹é¢„æµ‹çš„é‡è¦æ€§
if hasattr(meta_model, 'coef_'):
    meta_coef = meta_model.coef_
    n_base_models = len(base_models)
    base_model_coef = np.abs(meta_coef[:n_base_models])
    print("ç¬¬äºŒå±‚å…ƒæ¨¡å‹èµ‹äºˆå„åŸºæ¨¡å‹é¢„æµ‹çš„æƒé‡ (ç»å¯¹å€¼):")
    for i, (name, _) in enumerate(base_models):
        print(f"  {name:15}: {base_model_coef[i]:.4f}")