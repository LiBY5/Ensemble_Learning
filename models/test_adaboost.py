"""æµ‹è¯•AdaBoostå®ç° - ä¼˜åŒ–ç‰ˆæœ¬"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_regression, load_breast_cancer, load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import AdaBoostClassifier as SklearnAdaBoost
from sklearn.ensemble import AdaBoostRegressor as SklearnAdaBoostRegressor
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix, classification_report
import seaborn as sns
import os
import warnings
warnings.filterwarnings('ignore')

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs('../results/figures', exist_ok=True)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8-darkgrid')

# å¯¼å…¥æˆ‘ä»¬çš„å®ç°
import sys
sys.path.append('..')
from models.boosting import AdaBoostClassifier, AdaBoostRegressor


def test_adaboost_classifier():
    """æµ‹è¯•AdaBooståˆ†ç±»å™¨"""
    print("="*60)
    print("AdaBooståˆ†ç±»å™¨æµ‹è¯•")
    print("="*60)

    # ä½¿ç”¨çœŸå®æ•°æ®é›† - ä¹³è…ºç™Œæ•°æ®é›†
    from sklearn.datasets import load_breast_cancer
    data = load_breast_cancer()
    X, y = data.data, data.target

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ•°æ®é›†: ä¹³è…ºç™Œæ•°æ®é›†")
    print(f"  è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"  æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: è‰¯æ€§={np.sum(y==0)}, æ¶æ€§={np.sum(y==1)}")
    print(f"  ç‰¹å¾æ•°é‡: {X.shape[1]}")

    # æˆ‘ä»¬çš„å®ç°
    our_adaboost = AdaBoostClassifier(
        base_estimator=DecisionTreeClassifier(max_depth=3, random_state=42),
        n_estimators=50,
        learning_rate=1.0,
        algorithm='SAMME',
        random_state=42
    )

    # sklearnçš„å®ç°
    sklearn_adaboost = SklearnAdaBoost(
        estimator=DecisionTreeClassifier(max_depth=3, random_state=42),
        n_estimators=50,
        learning_rate=1.0,
        algorithm='SAMME',
        random_state=42
    )

    print("\n" + "="*60)
    print("è®­ç»ƒæˆ‘ä»¬çš„AdaBoost...")
    our_adaboost.fit(X_train, y_train)
    our_pred = our_adaboost.predict(X_test)
    our_acc = accuracy_score(y_test, our_pred)

    print(f"\næˆ‘ä»¬çš„å®ç°ç»“æœ:")
    print(f"  æµ‹è¯•å‡†ç¡®ç‡: {our_acc:.4f}")
    print(f"  å®é™…ä½¿ç”¨çš„åŸºå­¦ä¹ å™¨æ•°é‡: {our_adaboost.n_estimators}")

    if hasattr(our_adaboost, 'estimator_weights_'):
        weights = our_adaboost.estimator_weights_[:our_adaboost.n_estimators]
        if len(weights) > 0:
            print(f"  åŸºå­¦ä¹ å™¨æƒé‡èŒƒå›´: [{weights.min():.4f}, {weights.max():.4f}]")
            print(f"  å¹³å‡åŸºå­¦ä¹ å™¨æƒé‡: {weights.mean():.4f}")

    if hasattr(our_adaboost, 'estimator_errors_'):
        errors = our_adaboost.estimator_errors_[:our_adaboost.n_estimators]
        if len(errors) > 0:
            print(f"  åŸºå­¦ä¹ å™¨é”™è¯¯ç‡èŒƒå›´: [{errors.min():.4f}, {errors.max():.4f}]")
            print(f"  å¹³å‡åŸºå­¦ä¹ å™¨é”™è¯¯ç‡: {errors.mean():.4f}")

    print("\n" + "="*60)
    print("è®­ç»ƒsklearnçš„AdaBoost...")
    sklearn_adaboost.fit(X_train, y_train)
    sklearn_pred = sklearn_adaboost.predict(X_test)
    sklearn_acc = accuracy_score(y_test, sklearn_pred)

    print(f"\nsklearnå®ç°ç»“æœ:")
    print(f"  æµ‹è¯•å‡†ç¡®ç‡: {sklearn_acc:.4f}")
    print(f"  å®é™…ä½¿ç”¨çš„åŸºå­¦ä¹ å™¨æ•°é‡: {len(sklearn_adaboost.estimators_)}")

    # ç”Ÿæˆè¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("\n" + "="*60)
    print("æˆ‘ä»¬çš„å®ç°åˆ†ç±»æŠ¥å‘Š:")
    print("="*60)
    print(classification_report(y_test, our_pred,
                               target_names=['è‰¯æ€§', 'æ¶æ€§']))

    print("\n" + "="*60)
    print("sklearnå®ç°åˆ†ç±»æŠ¥å‘Š:")
    print("="*60)
    print(classification_report(y_test, sklearn_pred,
                               target_names=['è‰¯æ€§', 'æ¶æ€§']))

    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    visualize_adaboost_classifier_training(our_adaboost, X_train, y_train, X_test, y_test)

    # å¯¹æ¯”å¯è§†åŒ–
    visualize_classifier_comparison(our_adaboost, sklearn_adaboost, X_test, y_test)

    return our_acc, sklearn_acc, our_adaboost, sklearn_adaboost


def visualize_adaboost_classifier_training(model, X_train, y_train, X_test, y_test):
    """å¯è§†åŒ–AdaBooståˆ†ç±»å™¨è®­ç»ƒè¿‡ç¨‹"""
    print("\n" + "="*60)
    print("å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹...")

    train_errors = []
    test_errors = []

    # è·å–æ¯ä¸ªé˜¶æ®µçš„é¢„æµ‹
    for i, y_pred in enumerate(model.staged_predict(X_train), 1):
        train_errors.append(1 - accuracy_score(y_train, y_pred))

    for i, y_pred in enumerate(model.staged_predict(X_test), 1):
        test_errors.append(1 - accuracy_score(y_test, y_pred))

    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # 1. è¯¯å·®æ›²çº¿
    ax1 = axes[0, 0]
    ax1.plot(range(1, len(train_errors) + 1), train_errors,
             'b-', label='è®­ç»ƒè¯¯å·®', linewidth=2, alpha=0.8)
    ax1.plot(range(1, len(test_errors) + 1), test_errors,
             'r-', label='æµ‹è¯•è¯¯å·®', linewidth=2, alpha=0.8)
    ax1.set_xlabel('åŸºå­¦ä¹ å™¨æ•°é‡', fontsize=12)
    ax1.set_ylabel('è¯¯å·®', fontsize=12)
    ax1.set_title('AdaBoostè®­ç»ƒè¿‡ç¨‹ - è¯¯å·®æ›²çº¿', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # æ ‡è®°æœ€ä½³æµ‹è¯•è¯¯å·®
    if len(test_errors) > 0:
        best_idx = np.argmin(test_errors)
        ax1.axvline(x=best_idx + 1, color='g', linestyle='--',
                    label=f'æœ€ä½³: {best_idx+1}æ£µæ ‘', alpha=0.7)
        ax1.scatter(best_idx + 1, test_errors[best_idx],
                   color='g', s=100, zorder=5, edgecolor='black', linewidth=1)

    # 2. åŸºå­¦ä¹ å™¨æƒé‡
    ax2 = axes[0, 1]
    if hasattr(model, 'estimator_weights_'):
        weights = model.estimator_weights_[:model.n_estimators]
        if len(weights) > 0:
            bars = ax2.bar(range(1, len(weights) + 1), weights,
                          color='steelblue', edgecolor='navy', alpha=0.7)
            ax2.set_xlabel('åŸºå­¦ä¹ å™¨ç´¢å¼•', fontsize=12)
            ax2.set_ylabel('æƒé‡', fontsize=12)
            ax2.set_title('åŸºå­¦ä¹ å™¨æƒé‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')
            ax2.grid(True, alpha=0.3, axis='y')

            # æ·»åŠ å¹³å‡å€¼çº¿
            mean_weight = np.mean(weights)
            ax2.axhline(y=mean_weight, color='red', linestyle='--',
                       linewidth=2, alpha=0.7, label=f'å¹³å‡æƒé‡: {mean_weight:.4f}')
            ax2.legend()

    # 3. åŸºå­¦ä¹ å™¨é”™è¯¯ç‡
    ax3 = axes[1, 0]
    if hasattr(model, 'estimator_errors_'):
        errors = model.estimator_errors_[:model.n_estimators]
        if len(errors) > 0:
            ax3.bar(range(1, len(errors) + 1), errors,
                   color='lightcoral', edgecolor='darkred', alpha=0.7)
            ax3.set_xlabel('åŸºå­¦ä¹ å™¨ç´¢å¼•', fontsize=12)
            ax3.set_ylabel('é”™è¯¯ç‡', fontsize=12)
            ax3.set_title('åŸºå­¦ä¹ å™¨é”™è¯¯ç‡', fontsize=14, fontweight='bold')
            ax3.grid(True, alpha=0.3, axis='y')

    # 4. å¯¹æ•°å°ºåº¦è¯¯å·®æ›²çº¿
    ax4 = axes[1, 1]
    if len(train_errors) > 0 and len(test_errors) > 0:
        ax4.semilogy(range(1, len(train_errors) + 1), train_errors,
                    'b-', label='è®­ç»ƒè¯¯å·®', linewidth=2, alpha=0.8)
        ax4.semilogy(range(1, len(test_errors) + 1), test_errors,
                    'r-', label='æµ‹è¯•è¯¯å·®', linewidth=2, alpha=0.8)
        ax4.set_xlabel('åŸºå­¦ä¹ å™¨æ•°é‡', fontsize=12)
        ax4.set_ylabel('è¯¯å·®ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰', fontsize=12)
        ax4.set_title('AdaBoostè®­ç»ƒè¿‡ç¨‹ - å¯¹æ•°å°ºåº¦', fontsize=14, fontweight='bold')
        ax4.legend()
        ax4.grid(True, alpha=0.3, which='both')

    plt.suptitle('AdaBooståˆ†ç±»å™¨è®­ç»ƒè¿‡ç¨‹åˆ†æ', fontsize=16, fontweight='bold')
    plt.tight_layout()

    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

    plt.savefig('../results/figures/day3_adaboost_classifier_training.png', dpi=150, bbox_inches='tight')
    plt.show()

    # æ‰“å°åˆ†æç»“æœ
    if len(train_errors) > 0 and len(test_errors) > 0:
        print(f"\nè®­ç»ƒè¿‡ç¨‹åˆ†æ:")
        print(f"  æœ€ç»ˆè®­ç»ƒè¯¯å·®: {train_errors[-1]:.4f}")
        print(f"  æœ€ç»ˆæµ‹è¯•è¯¯å·®: {test_errors[-1]:.4f}")
        if len(test_errors) > 0:
            best_idx = np.argmin(test_errors)
            print(f"  æœ€ä½³æµ‹è¯•è¯¯å·®åœ¨ç¬¬{best_idx+1}æ£µæ ‘: {test_errors[best_idx]:.4f}")
        print(f"  è¿‡æ‹Ÿåˆç¨‹åº¦ï¼ˆæµ‹è¯•è¯¯å·®-è®­ç»ƒè¯¯å·®ï¼‰: {test_errors[-1] - train_errors[-1]:.4f}")


def visualize_classifier_comparison(our_model, sklearn_model, X_test, y_test):
    """å¯è§†åŒ–åˆ†ç±»å™¨å¯¹æ¯”"""
    our_pred = our_model.predict(X_test)
    sklearn_pred = sklearn_model.predict(X_test)

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    # 1. å‡†ç¡®ç‡å¯¹æ¯”
    ax1 = axes[0, 0]
    models = ['æˆ‘ä»¬çš„å®ç°', 'sklearnå®ç°']
    accuracies = [accuracy_score(y_test, our_pred),
                  accuracy_score(y_test, sklearn_pred)]

    colors = ['lightgreen', 'lightcoral']
    bars = ax1.bar(models, accuracies, color=colors, edgecolor='black')
    ax1.set_ylabel('å‡†ç¡®ç‡', fontsize=12)
    ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax1.set_ylim([0, 1])
    ax1.grid(True, alpha=0.3, axis='y')

    for bar, acc in zip(bars, accuracies):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{acc:.4f}', ha='center', va='bottom')

    # 2. æˆ‘ä»¬çš„å®ç°æ··æ·†çŸ©é˜µ
    ax2 = axes[0, 1]
    cm_our = confusion_matrix(y_test, our_pred)
    sns.heatmap(cm_our, annot=True, fmt='d', cmap='Blues',
                xticklabels=['é¢„æµ‹è‰¯æ€§', 'é¢„æµ‹æ¶æ€§'],
                yticklabels=['çœŸå®è‰¯æ€§', 'çœŸå®æ¶æ€§'], ax=ax2)
    ax2.set_title('æˆ‘ä»¬çš„å®ç° - æ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')

    # 3. sklearnå®ç°æ··æ·†çŸ©é˜µ
    ax3 = axes[0, 2]
    cm_sklearn = confusion_matrix(y_test, sklearn_pred)
    sns.heatmap(cm_sklearn, annot=True, fmt='d', cmap='Reds',
                xticklabels=['é¢„æµ‹è‰¯æ€§', 'é¢„æµ‹æ¶æ€§'],
                yticklabels=['çœŸå®è‰¯æ€§', 'çœŸå®æ¶æ€§'], ax=ax3)
    ax3.set_title('sklearnå®ç° - æ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')

    # 4. åŸºå­¦ä¹ å™¨æ•°é‡å¯¹æ¯”
    ax4 = axes[1, 0]
    n_estimators_our = our_model.n_estimators
    n_estimators_sklearn = len(sklearn_model.estimators_)

    bars = ax4.bar(['æˆ‘ä»¬çš„å®ç°', 'sklearnå®ç°'],
                  [n_estimators_our, n_estimators_sklearn],
                  color=['lightblue', 'lightpink'], edgecolor='black')
    ax4.set_ylabel('åŸºå­¦ä¹ å™¨æ•°é‡', fontsize=12)
    ax4.set_title('åŸºå­¦ä¹ å™¨æ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3, axis='y')

    for bar, n in zip(bars, [n_estimators_our, n_estimators_sklearn]):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2, height + 0.5,
                f'{n}', ha='center', va='bottom')

    # 5. å·®å¼‚åˆ†æ
    ax5 = axes[1, 1]
    diff = np.abs(our_pred - sklearn_pred)
    n_different = np.sum(diff)
    n_total = len(y_test)

    labels = ['é¢„æµ‹ä¸€è‡´', 'é¢„æµ‹ä¸åŒ']
    sizes = [n_total - n_different, n_different]
    colors = ['lightgreen', 'lightcoral']

    ax5.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
           startangle=90, wedgeprops={'edgecolor': 'black'})
    ax5.set_title(f'æ¨¡å‹é¢„æµ‹ä¸€è‡´æ€§\n(ä¸åŒ: {n_different}/{n_total})',
                 fontsize=14, fontweight='bold')

    # 6. é”™è¯¯æ ·æœ¬åˆ†æ
    ax6 = axes[1, 2]
    our_correct = (our_pred == y_test)
    sklearn_correct = (sklearn_pred == y_test)

    categories = ['ä¸¤è€…æ­£ç¡®', 'ä»…æˆ‘ä»¬æ­£ç¡®', 'ä»…sklearnæ­£ç¡®', 'ä¸¤è€…é”™è¯¯']
    counts = [
        np.sum(our_correct & sklearn_correct),
        np.sum(our_correct & ~sklearn_correct),
        np.sum(~our_correct & sklearn_correct),
        np.sum(~our_correct & ~sklearn_correct)
    ]

    bars = ax6.bar(categories, counts, color=['lightgreen', 'lightblue',
                                              'lightcoral', 'gray'])
    ax6.set_ylabel('æ ·æœ¬æ•°é‡', fontsize=12)
    ax6.set_title('æ¨¡å‹é”™è¯¯åˆ†æ', fontsize=14, fontweight='bold')
    ax6.tick_params(axis='x', rotation=45)
    ax6.grid(True, alpha=0.3, axis='y')

    for bar, count in zip(bars, counts):
        height = bar.get_height()
        ax6.text(bar.get_x() + bar.get_width()/2, height + 0.5,
                f'{count}', ha='center', va='bottom')

    plt.suptitle('AdaBooståˆ†ç±»å™¨å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

    plt.savefig('../results/figures/day3_adaboost_classifier_comparison.png',
                dpi=150, bbox_inches='tight')
    plt.show()

    # æ‰“å°å¯¹æ¯”åˆ†æ
    print("\n" + "="*60)
    print("æ¨¡å‹å¯¹æ¯”åˆ†æ")
    print("="*60)
    print(f"æˆ‘ä»¬çš„å®ç°å‡†ç¡®ç‡: {accuracies[0]:.4f}")
    print(f"sklearnå®ç°å‡†ç¡®ç‡: {accuracies[1]:.4f}")
    print(f"å‡†ç¡®ç‡å·®å¼‚: {abs(accuracies[0] - accuracies[1]):.4f}")
    print(f"ç›¸å¯¹å·®å¼‚: {abs(accuracies[0] - accuracies[1]) / accuracies[1] * 100:.2f}%")
    print(f"é¢„æµ‹ä¸€è‡´æ€§: {(n_total - n_different) / n_total * 100:.1f}%")
    print(f"å…±åŒé”™è¯¯çš„æ ·æœ¬æ•°: {counts[3]}")


def test_adaboost_regressor():
    """æµ‹è¯•AdaBoostå›å½’å™¨ - ä¿®å¤ç‰ˆæœ¬"""
    print("\n" + "=" * 60)
    print("AdaBoostå›å½’å™¨æµ‹è¯•")
    print("=" * 60)

    # ä½¿ç”¨çœŸå®æ•°æ®é›† - ç³–å°¿ç—…æ•°æ®é›†
    from sklearn.datasets import load_diabetes
    data = load_diabetes()
    X, y = data.data, data.target

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    print(f"æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ•°æ®é›†: ç³–å°¿ç—…æ•°æ®é›†")
    print(f"  è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"  æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    print(f"  ç›®æ ‡å€¼èŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
    print(f"  ç›®æ ‡å€¼å‡å€¼: {y.mean():.2f} Â± {y.std():.2f}")

    # æˆ‘ä»¬çš„å®ç°
    our_adaboost_reg = AdaBoostRegressor(
        base_estimator=DecisionTreeRegressor(max_depth=3, random_state=42),
        n_estimators=30,
        learning_rate=0.1,
        loss='square',
        random_state=42
    )

    # sklearnçš„å®ç°
    sklearn_adaboost_reg = SklearnAdaBoostRegressor(
        estimator=DecisionTreeRegressor(max_depth=3, random_state=42),
        n_estimators=30,
        learning_rate=0.1,
        loss='square',
        random_state=42
    )

    print("\nè®­ç»ƒæˆ‘ä»¬çš„AdaBoostå›å½’å™¨...")
    try:
        our_adaboost_reg.fit(X_train, y_train)
        our_pred = our_adaboost_reg.predict(X_test)
        our_mse = mean_squared_error(y_test, our_pred)
        our_r2 = r2_score(y_test, our_pred)

        print(f"  æˆåŠŸè®­ç»ƒäº† {len(our_adaboost_reg.estimators_)} ä¸ªåŸºå­¦ä¹ å™¨")
    except Exception as e:
        print(f"  è®­ç»ƒå¤±è´¥: {e}")
        # å¦‚æœè®­ç»ƒå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é¢„æµ‹
        our_pred = np.zeros_like(y_test)
        our_mse = mean_squared_error(y_test, our_pred)
        our_r2 = r2_score(y_test, our_pred)

    print("\nè®­ç»ƒsklearnçš„AdaBoostå›å½’å™¨...")
    sklearn_adaboost_reg.fit(X_train, y_train)
    sklearn_pred = sklearn_adaboost_reg.predict(X_test)
    sklearn_mse = mean_squared_error(y_test, sklearn_pred)
    sklearn_r2 = r2_score(y_test, sklearn_pred)

    print(f"\næˆ‘ä»¬çš„å®ç°ç»“æœ:")
    print(f"  æµ‹è¯•MSE: {our_mse:.4f}")
    print(f"  æµ‹è¯•RÂ²: {our_r2:.4f}")
    print(f"  RMSE: {np.sqrt(our_mse):.4f}")
    print(
        f"  å®é™…ä½¿ç”¨çš„åŸºå­¦ä¹ å™¨æ•°é‡: {len(our_adaboost_reg.estimators_) if hasattr(our_adaboost_reg, 'estimators_') else 0}")

    if hasattr(our_adaboost_reg, 'estimator_weights_') and len(our_adaboost_reg.estimator_weights_) > 0:
        weights = our_adaboost_reg.estimator_weights_
        print(f"  åŸºå­¦ä¹ å™¨æƒé‡èŒƒå›´: [{weights.min():.4f}, {weights.max():.4f}]")
        print(f"  å¹³å‡åŸºå­¦ä¹ å™¨æƒé‡: {weights.mean():.4f}")

    print(f"\nsklearnå®ç°ç»“æœ:")
    print(f"  æµ‹è¯•MSE: {sklearn_mse:.4f}")
    print(f"  æµ‹è¯•RÂ²: {sklearn_r2:.4f}")
    print(f"  RMSE: {np.sqrt(sklearn_mse):.4f}")
    print(f"  å®é™…ä½¿ç”¨çš„åŸºå­¦ä¹ å™¨æ•°é‡: {len(sklearn_adaboost_reg.estimators_)}")

    # å¯è§†åŒ–é¢„æµ‹ç»“æœ
    visualize_adaboost_regressor_results(
        our_adaboost_reg, sklearn_adaboost_reg,
        our_pred, sklearn_pred, y_test,
        our_mse, sklearn_mse, our_r2, sklearn_r2
    )

    return our_mse, our_r2, sklearn_mse, sklearn_r2


def visualize_adaboost_regressor_results(our_model, sklearn_model,
                                         our_pred, sklearn_pred, y_test,
                                         our_mse, sklearn_mse, our_r2, sklearn_r2):
    """å¯è§†åŒ–AdaBoostå›å½’å™¨ç»“æœ - ä¿®å¤ç‰ˆæœ¬"""
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    # 1. æˆ‘ä»¬çš„å®ç°é¢„æµ‹ç»“æœ
    ax1 = axes[0, 0]
    ax1.scatter(y_test, our_pred, alpha=0.6, edgecolors='k', linewidth=0.5, s=20)

    # è®¡ç®—å›å½’çº¿
    z = np.polyfit(y_test, our_pred, 1)
    p = np.poly1d(z)
    x_range = np.linspace(y_test.min(), y_test.max(), 100)
    ax1.plot(x_range, p(x_range), 'r--', linewidth=2,
             label=f'y = {z[0]:.3f}x + {z[1]:.3f}')

    # ç†æƒ³å¯¹è§’çº¿
    ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'g-', linewidth=2, alpha=0.5, label='ç†æƒ³é¢„æµ‹')

    ax1.set_xlabel('çœŸå®å€¼', fontsize=12)
    ax1.set_ylabel('é¢„æµ‹å€¼', fontsize=12)
    ax1.set_title('æˆ‘ä»¬çš„å®ç° - é¢„æµ‹ç»“æœ', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. sklearnå®ç°é¢„æµ‹ç»“æœ
    ax2 = axes[0, 1]
    ax2.scatter(y_test, sklearn_pred, alpha=0.6, edgecolors='k', linewidth=0.5, s=20)

    z = np.polyfit(y_test, sklearn_pred, 1)
    p = np.poly1d(z)
    ax2.plot(x_range, p(x_range), 'r--', linewidth=2,
             label=f'y = {z[0]:.3f}x + {z[1]:.3f}')

    ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'g-', linewidth=2, alpha=0.5, label='ç†æƒ³é¢„æµ‹')

    ax2.set_xlabel('çœŸå®å€¼', fontsize=12)
    ax2.set_ylabel('é¢„æµ‹å€¼', fontsize=12)
    ax2.set_title('sklearnå®ç° - é¢„æµ‹ç»“æœ', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. æ®‹å·®å¯¹æ¯”
    ax3 = axes[0, 2]
    our_residuals = y_test - our_pred
    sklearn_residuals = y_test - sklearn_pred

    bins = 30
    ax3.hist(our_residuals, bins=bins, alpha=0.7, color='blue',
             label='æˆ‘ä»¬çš„å®ç°', edgecolor='black', density=True)
    ax3.hist(sklearn_residuals, bins=bins, alpha=0.7, color='red',
             label='sklearnå®ç°', edgecolor='black', density=True)

    ax3.axvline(x=0, color='green', linestyle='--', linewidth=2,
                label='é›¶æ®‹å·®çº¿', alpha=0.7)
    ax3.set_xlabel('æ®‹å·®', fontsize=12)
    ax3.set_ylabel('æ¦‚ç‡å¯†åº¦', fontsize=12)
    ax3.set_title('æ®‹å·®åˆ†å¸ƒå¯¹æ¯”ï¼ˆå½’ä¸€åŒ–ï¼‰', fontsize=14, fontweight='bold')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. åŸºå­¦ä¹ å™¨æƒé‡åˆ†å¸ƒï¼ˆæˆ‘ä»¬çš„å®ç°ï¼‰
    ax4 = axes[1, 0]
    if hasattr(our_model, 'estimator_weights_') and len(our_model.estimator_weights_) > 0:
        weights = our_model.estimator_weights_
        bars = ax4.bar(range(1, len(weights) + 1), weights,
                       color='steelblue', edgecolor='navy', alpha=0.7)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_weight = np.mean(weights)
        median_weight = np.median(weights)
        ax4.axhline(y=mean_weight, color='red', linestyle='--',
                    linewidth=2, label=f'å‡å€¼: {mean_weight:.4f}', alpha=0.7)
        ax4.axhline(y=median_weight, color='orange', linestyle='--',
                    linewidth=2, label=f'ä¸­ä½æ•°: {median_weight:.4f}', alpha=0.7)

        ax4.set_xlabel('åŸºå­¦ä¹ å™¨ç´¢å¼•', fontsize=12)
        ax4.set_ylabel('æƒé‡', fontsize=12)
        ax4.set_title(f'æˆ‘ä»¬çš„å®ç° - åŸºå­¦ä¹ å™¨æƒé‡\n(n={len(weights)})', fontsize=14, fontweight='bold')
        ax4.legend(fontsize=9)
        ax4.grid(True, alpha=0.3, axis='y')
    else:
        ax4.text(0.5, 0.5, 'æ— åŸºå­¦ä¹ å™¨æƒé‡æ•°æ®',
                 ha='center', va='center', fontsize=12,
                 transform=ax4.transAxes)
        ax4.set_title('æˆ‘ä»¬çš„å®ç° - åŸºå­¦ä¹ å™¨æƒé‡', fontsize=14, fontweight='bold')

    # 5. è®­ç»ƒæŸå¤±æ›²çº¿
    ax5 = axes[1, 1]
    if hasattr(our_model, 'train_scores_') and len(our_model.train_scores_) > 0:
        train_scores = our_model.train_scores_
        ax5.plot(range(1, len(train_scores) + 1), train_scores,
                 'b-', linewidth=2, label='è®­ç»ƒæŸå¤±', alpha=0.8)
        ax5.set_xlabel('è¿­ä»£æ¬¡æ•°', fontsize=12)
        ax5.set_ylabel('MSEæŸå¤±', fontsize=12)
        ax5.set_title('è®­ç»ƒè¿‡ç¨‹æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
        ax5.grid(True, alpha=0.3)
        ax5.legend()
    else:
        ax5.text(0.5, 0.5, 'æ— è®­ç»ƒæŸå¤±æ•°æ®',
                 ha='center', va='center', fontsize=12,
                 transform=ax5.transAxes)
        ax5.set_title('è®­ç»ƒè¿‡ç¨‹æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')

    # 6. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
    ax6 = axes[1, 2]
    metrics = ['MSE', 'RÂ²', 'RMSE']
    our_scores = [our_mse, our_r2, np.sqrt(our_mse)]
    sklearn_scores = [sklearn_mse, sklearn_r2, np.sqrt(sklearn_mse)]

    x = np.arange(len(metrics))
    width = 0.35

    bars1 = ax6.bar(x - width / 2, our_scores, width, label='æˆ‘ä»¬çš„å®ç°',
                    color='lightblue', edgecolor='navy')
    bars2 = ax6.bar(x + width / 2, sklearn_scores, width, label='sklearnå®ç°',
                    color='lightcoral', edgecolor='darkred')

    ax6.set_xlabel('æŒ‡æ ‡', fontsize=12)
    ax6.set_ylabel('å€¼', fontsize=12)
    ax6.set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax6.set_xticks(x)
    ax6.set_xticklabels(metrics)
    ax6.legend()
    ax6.grid(True, alpha=0.3, axis='y')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    max_score = max(max(our_scores), max(sklearn_scores))
    for bars, scores in zip([bars1, bars2], [our_scores, sklearn_scores]):
        for bar, score in zip(bars, scores):
            height = bar.get_height()
            # æ ¹æ®å€¼çš„å¤§å°è°ƒæ•´æ ‡ç­¾ä½ç½®
            label_y = height + 0.05 * max_score
            ax6.text(bar.get_x() + bar.get_width() / 2, label_y,
                     f'{score:.3f}', ha='center', va='bottom', fontsize=9)

    plt.suptitle('AdaBoostå›å½’å™¨ç»“æœå¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

    plt.savefig('../results/figures/day3_adaboost_regressor_comparison.png',
                dpi=150, bbox_inches='tight')
    plt.show()

    # æ‰“å°æ®‹å·®ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("æ®‹å·®ç»Ÿè®¡åˆ†æ")
    print("=" * 60)
    print(f"\næˆ‘ä»¬çš„å®ç°æ®‹å·®ç»Ÿè®¡:")
    print(f"  æ®‹å·®å‡å€¼: {np.mean(our_residuals):.4f}")
    print(f"  æ®‹å·®æ ‡å‡†å·®: {np.std(our_residuals):.4f}")
    print(f"  æ®‹å·®èŒƒå›´: [{our_residuals.min():.4f}, {our_residuals.max():.4f}]")

    print(f"\nsklearnå®ç°æ®‹å·®ç»Ÿè®¡:")
    print(f"  æ®‹å·®å‡å€¼: {np.mean(sklearn_residuals):.4f}")
    print(f"  æ®‹å·®æ ‡å‡†å·®: {np.std(sklearn_residuals):.4f}")
    print(f"  æ®‹å·®èŒƒå›´: [{sklearn_residuals.min():.4f}, {sklearn_residuals.max():.4f}]")

    print(f"\næ¨¡å‹å¯¹æ¯”:")
    print(f"  MSEå·®å¼‚: {abs(our_mse - sklearn_mse):.4f}")
    print(f"  RÂ²å·®å¼‚: {abs(our_r2 - sklearn_r2):.4f}")

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    print("å¼€å§‹AdaBoostç®—æ³•æµ‹è¯•...")
    print("="*60)

    try:
        # æµ‹è¯•åˆ†ç±»å™¨
        print("\n\næµ‹è¯•åˆ†ç±»å™¨...")
        our_acc, sklearn_acc, our_clf, sklearn_clf = test_adaboost_classifier()

        # æµ‹è¯•å›å½’å™¨
        print("\n\næµ‹è¯•å›å½’å™¨...")
        our_mse, our_r2, sklearn_mse, sklearn_r2 = test_adaboost_regressor()

        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        print("\n" + "="*60)
        print("æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print("="*60)

        print(f"\nåˆ†ç±»ä»»åŠ¡:")
        print(f"  æˆ‘ä»¬çš„å®ç°å‡†ç¡®ç‡: {our_acc:.4f}")
        print(f"  sklearnå®ç°å‡†ç¡®ç‡: {sklearn_acc:.4f}")
        print(f"  å‡†ç¡®ç‡å·®å¼‚: {abs(our_acc - sklearn_acc):.4f}")
        print(f"  ç›¸å¯¹å‡†ç¡®ç‡: {our_acc / sklearn_acc * 100:.2f}%")

        print(f"\nå›å½’ä»»åŠ¡:")
        print(f"  æˆ‘ä»¬çš„å®ç°MSE: {our_mse:.4f} (RÂ²: {our_r2:.4f})")
        print(f"  sklearnå®ç°MSE: {sklearn_mse:.4f} (RÂ²: {sklearn_r2:.4f})")
        print(f"  MSEç›¸å¯¹è¡¨ç°: {our_mse / sklearn_mse:.4f}")
        print(f"  RÂ²ç›¸å¯¹è¡¨ç°: {our_r2 / sklearn_r2:.4f}")

        print(f"\næ•´ä½“è¯„ä»·:")
        if our_acc > sklearn_acc and our_mse < sklearn_mse:
            print("  âœ… æˆ‘ä»¬çš„å®ç°åœ¨åˆ†ç±»å’Œå›å½’ä»»åŠ¡ä¸Šéƒ½ä¼˜äºsklearnå®ç°ï¼")
        elif our_acc > sklearn_acc:
            print("  ğŸ“ˆ æˆ‘ä»¬çš„å®ç°åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šä¼˜äºsklearnå®ç°")
        elif our_mse < sklearn_mse:
            print("  ğŸ“ˆ æˆ‘ä»¬çš„å®ç°åœ¨å›å½’ä»»åŠ¡ä¸Šä¼˜äºsklearnå®ç°")
        else:
            print("  ğŸ“Š æˆ‘ä»¬çš„å®ç°ä¸sklearnå®ç°æ€§èƒ½æ¥è¿‘")

        print(f"\næµ‹è¯•å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° ../results/figures/ ç›®å½•")

    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()