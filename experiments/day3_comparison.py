"""Bagging vs Boostingç»¼åˆå¯¹æ¯”å®éªŒï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import load_breast_cancer, load_diabetes
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import (
    RandomForestClassifier,
    AdaBoostClassifier,
    GradientBoostingClassifier,
    BaggingClassifier,
    BaggingRegressor,  # ä¿®å¤ï¼šå¯¼å…¥BaggingRegressor
    RandomForestRegressor,
    AdaBoostRegressor,
    GradientBoostingRegressor
)
from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, r2_score
import seaborn as sns
import os
import sys
import time

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥modelsæ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æˆ‘ä»¬çš„å®ç°
from models.boosting import AdaBoostClassifier as OurAdaBoost
from models.boosting import GradientBoostingClassifier as OurGBC
from models.boosting import GradientBoostingRegressor as OurGBR

def compare_classification_methods():
    """å¯¹æ¯”æ‰€æœ‰åˆ†ç±»é›†æˆæ–¹æ³•"""
    print("="*60)
    print("Bagging vs Boostingåˆ†ç±»æ–¹æ³•å¯¹æ¯”")
    print("="*60)

    # åŠ è½½ä¹³è…ºç™Œæ•°æ®é›†
    data = load_breast_cancer()
    X, y = data.data, data.target
    feature_names = data.feature_names

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ•°æ®é›†: ä¹³è…ºç™Œæ•°æ®é›†")
    print(f"  è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"  æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    print(f"  ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_train)}")
    print(f"  ç‰¹å¾æ•°é‡: {len(feature_names)}")

    # å®šä¹‰æ‰€æœ‰æ¨¡å‹
    models = {
        'å†³ç­–æ ‘ (åŸºçº¿)': DecisionTreeClassifier(max_depth=5, random_state=42),
        'Bagging (å†³ç­–æ ‘)': BaggingClassifier(
            estimator=DecisionTreeClassifier(max_depth=5, random_state=42),
            n_estimators=50,
            random_state=42
        ),
        'éšæœºæ£®æ—': RandomForestClassifier(
            n_estimators=50,
            max_depth=5,
            random_state=42
        ),
        'AdaBoost (æˆ‘ä»¬çš„å®ç°)': OurAdaBoost(
            base_estimator=DecisionTreeClassifier(max_depth=3, random_state=42),
            n_estimators=50,
            learning_rate=1.0,
            random_state=42
        ),
        'AdaBoost (sklearn)': AdaBoostClassifier(
            estimator=DecisionTreeClassifier(max_depth=3, random_state=42),
            n_estimators=50,
            learning_rate=1.0,
            random_state=42
        ),
        'GBDT (æˆ‘ä»¬çš„å®ç°)': OurGBC(
            loss='deviance',
            learning_rate=0.1,
            n_estimators=50,
            max_depth=3,
            random_state=42
        ),
        'GBDT (sklearn)': GradientBoostingClassifier(
            loss='log_loss',
            learning_rate=0.1,
            n_estimators=50,
            max_depth=3,
            random_state=42
        )
    }

    # è®­ç»ƒå’Œè¯„ä¼°
    results = []

    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")

        # è®­ç»ƒ
        start_time = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start_time

        # é¢„æµ‹
        y_pred = model.predict(X_test)

        # è®¡ç®—æŒ‡æ ‡
        acc = accuracy_score(y_test, y_pred)

        # å¯¹äºæ”¯æŒæ¦‚ç‡çš„æ¨¡å‹è®¡ç®—AUC
        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_test)[:, 1]
            auc = roc_auc_score(y_test, y_proba)
        else:
            auc = None

        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

        # è®°å½•ç»“æœ
        result = {
            'Model': name,
            'Accuracy': acc,
            'AUC': auc if auc is not None else np.nan,
            'CV_Mean': cv_scores.mean(),
            'CV_Std': cv_scores.std(),
            'Train_Time': train_time
        }

        results.append(result)

        print(f"  å‡†ç¡®ç‡: {acc:.4f}")
        if auc is not None:
            print(f"  AUC: {auc:.4f}")
        print(f"  äº¤å‰éªŒè¯: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        print(f"  è®­ç»ƒæ—¶é—´: {train_time:.3f}s")

    # è½¬æ¢ä¸ºDataFrame
    df_results = pd.DataFrame(results)

    # å¯è§†åŒ–å¯¹æ¯”
    visualize_classification_comparison(df_results, feature_names, models['éšæœºæ£®æ—'])

    return df_results, models

def compare_regression_methods():
    """å¯¹æ¯”æ‰€æœ‰å›å½’é›†æˆæ–¹æ³•ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
    print("\n" + "="*60)
    print("Bagging vs Boostingå›å½’æ–¹æ³•å¯¹æ¯”")
    print("="*60)

    # åŠ è½½ç³–å°¿ç—…æ•°æ®é›†
    data = load_diabetes()
    X, y = data.data, data.target

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    print(f"æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ•°æ®é›†: ç³–å°¿ç—…æ•°æ®é›†")
    print(f"  è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"  æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    print(f"  ç›®æ ‡å€¼èŒƒå›´: [{y.min():.2f}, {y.max():.2f}]")
    print(f"  ç›®æ ‡å€¼å‡å€¼: {y.mean():.2f}, æ ‡å‡†å·®: {y.std():.2f}")

    # å®šä¹‰æ‰€æœ‰å›å½’æ¨¡å‹ - ä¿®å¤ï¼šä½¿ç”¨BaggingRegressor
    models = {
        'å†³ç­–æ ‘ (åŸºçº¿)': DecisionTreeRegressor(max_depth=5, random_state=42),
        'Bagging (å†³ç­–æ ‘)': BaggingRegressor(  # ä¿®å¤ï¼šæ”¹ä¸ºBaggingRegressor
            estimator=DecisionTreeRegressor(max_depth=5, random_state=42),
            n_estimators=50,
            random_state=42
        ),
        'éšæœºæ£®æ—': RandomForestRegressor(
            n_estimators=50,
            max_depth=5,
            random_state=42
        ),
        'AdaBoost (sklearn)': AdaBoostRegressor(
            estimator=DecisionTreeRegressor(max_depth=3, random_state=42),
            n_estimators=50,
            learning_rate=1.0,
            random_state=42
        ),
        'GBDT (æˆ‘ä»¬çš„å®ç°)': OurGBR(
            loss='ls',
            learning_rate=0.1,
            n_estimators=50,
            max_depth=3,
            random_state=42
        ),
        'GBDT (sklearn)': GradientBoostingRegressor(
            loss='squared_error',
            learning_rate=0.1,
            n_estimators=50,
            max_depth=3,
            random_state=42
        )
    }

    # è®­ç»ƒå’Œè¯„ä¼°
    results = []

    for name, model in models.items():
        print(f"\nè®­ç»ƒ {name}...")

        # è®­ç»ƒ
        start_time = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start_time

        # é¢„æµ‹
        y_pred = model.predict(X_test)

        # è®¡ç®—æŒ‡æ ‡
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        # è®°å½•ç»“æœ
        result = {
            'Model': name,
            'MSE': mse,
            'RMSE': np.sqrt(mse),
            'RÂ²': r2,
            'Train_Time': train_time
        }
        results.append(result)

        print(f"  MSE: {mse:.4f}")
        print(f"  RMSE: {np.sqrt(mse):.4f}")
        print(f"  RÂ²: {r2:.4f}")
        print(f"  è®­ç»ƒæ—¶é—´: {train_time:.3f}s")

    # è½¬æ¢ä¸ºDataFrame
    df_results = pd.DataFrame(results)

    # å¯è§†åŒ–å¯¹æ¯”
    visualize_regression_comparison(df_results)

    return df_results

# å…¶ä½™å‡½æ•°ä¿æŒä¸å˜ï¼ˆvisualize_classification_comparison, visualize_regression_comparison,
# create_selection_guide, create_decision_flowchartï¼‰

def visualize_classification_comparison(df_results, feature_names, random_forest_model):
    """å¯è§†åŒ–åˆ†ç±»å¯¹æ¯”ç»“æœ"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))

    # 1. å‡†ç¡®ç‡å¯¹æ¯”
    ax1 = axes[0, 0]
    models_list = df_results['Model']
    accuracies = df_results['Accuracy']

    bars = ax1.barh(range(len(models_list)), accuracies, color='skyblue')
    ax1.set_yticks(range(len(models_list)))
    ax1.set_yticklabels(models_list)
    ax1.set_xlabel('å‡†ç¡®ç‡')
    ax1.set_title('ä¸åŒé›†æˆæ–¹æ³•çš„å‡†ç¡®ç‡å¯¹æ¯”')
    ax1.invert_yaxis()
    ax1.set_xlim([0.85, 1.0])

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, acc) in enumerate(zip(bars, accuracies)):
        ax1.text(acc + 0.002, i, f'{acc:.4f}', va='center')

    # 2. äº¤å‰éªŒè¯ç»“æœ
    ax2 = axes[0, 1]
    x_pos = range(len(models_list))
    ax2.errorbar(x_pos, df_results['CV_Mean'], yerr=df_results['CV_Std'],
                fmt='o', capsize=5, linewidth=2, markersize=8)
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(models_list, rotation=45, ha='right')
    ax2.set_ylabel('äº¤å‰éªŒè¯å‡†ç¡®ç‡')
    ax2.set_title('äº¤å‰éªŒè¯ç»“æœï¼ˆå‡å€¼Â±æ ‡å‡†å·®ï¼‰')
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim([0.85, 1.0])

    # 3. è®­ç»ƒæ—¶é—´å¯¹æ¯”
    ax3 = axes[1, 0]
    train_times = df_results['Train_Time']
    colors = ['lightgreen' if 'æˆ‘ä»¬çš„' in name else 'lightcoral' for name in models_list]
    bars = ax3.bar(range(len(models_list)), train_times, color=colors)
    ax3.set_xticks(range(len(models_list)))
    ax3.set_xticklabels(models_list, rotation=45, ha='right')
    ax3.set_ylabel('è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰')
    ax3.set_title('è®­ç»ƒæ—¶é—´å¯¹æ¯”')
    ax3.grid(True, alpha=0.3, axis='y')

    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='lightgreen', edgecolor='black', label='æˆ‘ä»¬çš„å®ç°'),
        Patch(facecolor='lightcoral', edgecolor='black', label='sklearnå®ç°')
    ]
    ax3.legend(handles=legend_elements, loc='upper left')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, t in zip(bars, train_times):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{t:.3f}', ha='center', va='bottom', fontsize=8)

    # 4. éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§
    ax4 = axes[1, 1]
    if hasattr(random_forest_model, 'feature_importances_'):
        importances = random_forest_model.feature_importances_
        indices = np.argsort(importances)[::-1][:10]  # åªæ˜¾ç¤ºå‰10ä¸ªç‰¹å¾

        # è·å–ç‰¹å¾åç§°
        feature_names_short = []
        for i in indices:
            if i < len(feature_names):
                # ç¼©çŸ­é•¿ç‰¹å¾å
                name = feature_names[i]
                if len(name) > 20:
                    name = name[:18] + '..'
                feature_names_short.append(name)
            else:
                feature_names_short.append(f'ç‰¹å¾{i}')

        ax4.bar(range(len(indices)), importances[indices], color='steelblue')
        ax4.set_xticks(range(len(indices)))
        ax4.set_xticklabels(feature_names_short, rotation=45, ha='right', fontsize=9)
        ax4.set_ylabel('é‡è¦æ€§')
        ax4.set_title('éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§ (Top 10)')
        ax4.grid(True, alpha=0.3, axis='y')

    plt.suptitle('é›†æˆå­¦ä¹ æ–¹æ³•ç»¼åˆå¯¹æ¯” (åˆ†ç±»ä»»åŠ¡)', fontsize=16, fontweight='bold')
    plt.tight_layout()
    # è®¾ç½®å­—ä½“ä¸ºç³»ç»Ÿè‡ªå¸¦çš„ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'KaiTi', 'FangSong']  # è®¾ç½®ä¸­æ–‡å­—ä½“
    # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
    plt.savefig('../results/figures/day3_classification_integration_comparison.png',
                dpi=150, bbox_inches='tight')
    plt.show()

    # æ‰“å°åˆ†ææ€»ç»“
    print("\n" + "="*60)
    print("åˆ†ææ€»ç»“")
    print("="*60)

    print(f"æœ€ä½³å‡†ç¡®ç‡: {df_results['Accuracy'].max():.4f} ({df_results.loc[df_results['Accuracy'].idxmax(), 'Model']})")
    print(f"æœ€ç¨³å®šæ¨¡å‹: {df_results.loc[df_results['CV_Std'].idxmin(), 'Model']} (CVæ ‡å‡†å·®: {df_results['CV_Std'].min():.4f})")
    print(f"æœ€å¿«è®­ç»ƒæ¨¡å‹: {df_results.loc[df_results['Train_Time'].idxmin(), 'Model']} ({df_results['Train_Time'].min():.3f}s)")

    # æ¨èæ¨¡å‹
    # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼šå‡†ç¡®ç‡æƒé‡0.6ï¼Œç¨³å®šæ€§æƒé‡0.3ï¼Œé€Ÿåº¦æƒé‡0.1
    df_results['Composite_Score'] = (
        0.6 * (df_results['Accuracy'] - df_results['Accuracy'].min()) / (df_results['Accuracy'].max() - df_results['Accuracy'].min()) +
        0.3 * (1 - (df_results['CV_Std'] - df_results['CV_Std'].min()) / (df_results['CV_Std'].max() - df_results['CV_Std'].min())) +
        0.1 * (1 - (df_results['Train_Time'] - df_results['Train_Time'].min()) / (df_results['Train_Time'].max() - df_results['Train_Time'].min()))
    )

    best_overall = df_results.loc[df_results['Composite_Score'].idxmax()]
    print(f"\næ¨èæ¨¡å‹: {best_overall['Model']} (ç»¼åˆè¯„åˆ†æœ€é«˜: {best_overall['Composite_Score']:.4f})")
    print(f"  å‡†ç¡®ç‡: {best_overall['Accuracy']:.4f}")
    print(f"  ç¨³å®šæ€§(CVæ ‡å‡†å·®): {best_overall['CV_Std']:.4f}")
    print(f"  è®­ç»ƒæ—¶é—´: {best_overall['Train_Time']:.3f}s")

def visualize_regression_comparison(df_results):
    """å¯è§†åŒ–å›å½’å¯¹æ¯”ç»“æœ"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))

    # 1. MSEå¯¹æ¯”
    ax1 = axes[0, 0]
    models_list = df_results['Model']
    mse_values = df_results['MSE']

    colors = ['lightgreen' if 'æˆ‘ä»¬çš„' in name else 'lightcoral' for name in models_list]
    bars = ax1.barh(range(len(models_list)), mse_values, color=colors)
    ax1.set_yticks(range(len(models_list)))
    ax1.set_yticklabels(models_list)
    ax1.set_xlabel('MSE (è¶Šå°è¶Šå¥½)')
    ax1.set_title('ä¸åŒé›†æˆæ–¹æ³•çš„MSEå¯¹æ¯”')
    ax1.invert_yaxis()

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, mse) in enumerate(zip(bars, mse_values)):
        ax1.text(mse + 5, i, f'{mse:.1f}', va='center')

    # æ·»åŠ å›¾ä¾‹
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='lightgreen', edgecolor='black', label='æˆ‘ä»¬çš„å®ç°'),
        Patch(facecolor='lightcoral', edgecolor='black', label='sklearnå®ç°')
    ]
    ax1.legend(handles=legend_elements, loc='lower right')

    # 2. RÂ²å¯¹æ¯”
    ax2 = axes[0, 1]
    r2_values = df_results['RÂ²']

    bars = ax2.bar(range(len(models_list)), r2_values, color='lightblue')
    ax2.set_xticks(range(len(models_list)))
    ax2.set_xticklabels(models_list, rotation=45, ha='right')
    ax2.set_ylabel('RÂ² (è¶Šå¤§è¶Šå¥½)')
    ax2.set_title('ä¸åŒé›†æˆæ–¹æ³•çš„RÂ²å¯¹æ¯”')
    ax2.grid(True, alpha=0.3, axis='y')
    ax2.set_ylim([0, 1])

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, r2 in zip(bars, r2_values):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{r2:.4f}', ha='center', va='bottom')

    # 3. è®­ç»ƒæ—¶é—´å¯¹æ¯”
    ax3 = axes[1, 0]
    train_times = df_results['Train_Time']
    colors = ['lightgreen' if 'æˆ‘ä»¬çš„' in name else 'lightcoral' for name in models_list]
    bars = ax3.bar(range(len(models_list)), train_times, color=colors)
    ax3.set_xticks(range(len(models_list)))
    ax3.set_xticklabels(models_list, rotation=45, ha='right')
    ax3.set_ylabel('è®­ç»ƒæ—¶é—´ï¼ˆç§’ï¼‰')
    ax3.set_title('è®­ç»ƒæ—¶é—´å¯¹æ¯”')
    ax3.grid(True, alpha=0.3, axis='y')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, t in zip(bars, train_times):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{t:.3f}', ha='center', va='bottom')

    # 4. æ¨¡å‹æ€§èƒ½ç»¼åˆè¯„ä»·
    ax4 = axes[1, 1]

    # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆMSEè¶Šå°è¶Šå¥½ï¼ŒRÂ²è¶Šå¤§è¶Šå¥½ï¼Œæ—¶é—´è¶ŠçŸ­è¶Šå¥½ï¼‰
    # å½’ä¸€åŒ–å¤„ç†
    mse_norm = 1 - (df_results['MSE'] - df_results['MSE'].min()) / (df_results['MSE'].max() - df_results['MSE'].min())
    r2_norm = (df_results['RÂ²'] - df_results['RÂ²'].min()) / (df_results['RÂ²'].max() - df_results['RÂ²'].min())
    time_norm = 1 - (df_results['Train_Time'] - df_results['Train_Time'].min()) / (df_results['Train_Time'].max() - df_results['Train_Time'].min())

    # ç»¼åˆè¯„åˆ† = 0.4 * MSEè¯„åˆ† + 0.4 * RÂ²è¯„åˆ† + 0.2 * æ—¶é—´è¯„åˆ†
    composite_score = 0.4 * mse_norm + 0.4 * r2_norm + 0.2 * time_norm

    colors = ['lightgreen' if 'æˆ‘ä»¬çš„' in name else 'lightcoral' for name in models_list]
    bars = ax4.bar(range(len(models_list)), composite_score, color=colors)
    ax4.set_xticks(range(len(models_list)))
    ax4.set_xticklabels(models_list, rotation=45, ha='right')
    ax4.set_ylabel('ç»¼åˆè¯„åˆ† (0-1)')
    ax4.set_title('æ¨¡å‹æ€§èƒ½ç»¼åˆè¯„ä»·')
    ax4.grid(True, alpha=0.3, axis='y')
    ax4.set_ylim([0, 1])

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, score in zip(bars, composite_score):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2, height + 0.01,
                f'{score:.4f}', ha='center', va='bottom')

    plt.suptitle('é›†æˆå­¦ä¹ æ–¹æ³•ç»¼åˆå¯¹æ¯” (å›å½’ä»»åŠ¡)', fontsize=16, fontweight='bold')
    plt.tight_layout()
    # è®¾ç½®å­—ä½“ä¸ºç³»ç»Ÿè‡ªå¸¦çš„ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'KaiTi', 'FangSong']  # è®¾ç½®ä¸­æ–‡å­—ä½“
    # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
    plt.savefig('../results/figures/day3_regression_integration_comparison.png',
                dpi=150, bbox_inches='tight')
    plt.show()

    # æ‰“å°åˆ†ææ€»ç»“
    print("\n" + "="*60)
    print("åˆ†ææ€»ç»“")
    print("="*60)

    print(f"æœ€ä½³MSE: {df_results['MSE'].min():.4f} ({df_results.loc[df_results['MSE'].idxmin(), 'Model']})")
    print(f"æœ€ä½³RÂ²: {df_results['RÂ²'].max():.4f} ({df_results.loc[df_results['RÂ²'].idxmax(), 'Model']})")

    # æ‰¾å‡ºç»¼åˆè¯„åˆ†æœ€é«˜çš„æ¨¡å‹
    df_results['Composite_Score'] = composite_score
    best_composite_idx = composite_score.idxmax()
    print(f"\næ¨èæ¨¡å‹: {df_results.loc[best_composite_idx, 'Model']}")
    print(f"  ç»¼åˆè¯„åˆ†: {composite_score[best_composite_idx]:.4f}")
    print(f"  MSE: {df_results.loc[best_composite_idx, 'MSE']:.4f}")
    print(f"  RÂ²: {df_results.loc[best_composite_idx, 'RÂ²']:.4f}")
    print(f"  è®­ç»ƒæ—¶é—´: {df_results.loc[best_composite_idx, 'Train_Time']:.3f}s")

def create_selection_guide():
    """åˆ›å»ºé›†æˆæ–¹æ³•é€‰æ‹©æŒ‡å—"""
    print("\n" + "="*60)
    print("é›†æˆæ–¹æ³•é€‰æ‹©æŒ‡å—")
    print("="*60)

    guide = {
        'é—®é¢˜ç±»å‹': {
            'åˆ†ç±»': {
                'é«˜ç»´æ•°æ®ï¼Œå™ªå£°å¤§': 'éšæœºæ£®æ— (å¹¶è¡Œè®­ç»ƒï¼ŒæŠ—å™ªèƒ½åŠ›å¼º)',
                'ç‰¹å¾æ•°å°‘ï¼Œéœ€è¦å¼ºè§£é‡Šæ€§': 'GBDT (ç‰¹å¾é‡è¦æ€§æ›´ç²¾ç¡®)',
                'äºŒåˆ†ç±»ï¼ŒåŸºæ¨¡å‹ç®€å•': 'AdaBoost (å¯¹å¼±å­¦ä¹ å™¨æœ‰æ•ˆ)',
                'éœ€è¦å¿«é€ŸåŸå‹': 'Bagging (å¹¶è¡Œï¼Œè®­ç»ƒå¿«)'
            },
            'å›å½’': {
                'æ•°æ®æœ‰å¼‚å¸¸å€¼': 'GBDT (HuberæŸå¤±) æˆ– AdaBoost (ç»å¯¹æŸå¤±)',
                'éœ€è¦ç²¾ç¡®é¢„æµ‹': 'GBDT (å¹³æ–¹æŸå¤±)',
                'è®¡ç®—èµ„æºå……è¶³': 'éšæœºæ£®æ—',
                'éœ€è¦ç¨³å¥æ¨¡å‹': 'Bagging'
            }
        },
        'æ•°æ®ç‰¹å¾': {
            'ç‰¹å¾å¤šï¼Œæ ·æœ¬å°‘': 'éšæœºæ£®æ—æˆ–GBDT',
            'å™ªå£°å¤§': 'éšæœºæ£®æ— (ç‰¹å¾é‡‡æ ·å‡å°‘å™ªå£°å½±å“)',
            'ç±»åˆ«ä¸å¹³è¡¡': 'AdaBoost (è‡ªé€‚åº”æƒé‡è°ƒæ•´)',
            'ç‰¹å¾é—´æœ‰äº¤äº’': 'GBDT (è‡ªåŠ¨å­¦ä¹ äº¤äº’ç‰¹å¾)'
        },
        'è®¡ç®—èµ„æº': {
            'è®¡ç®—èµ„æºå……è¶³': 'éšæœºæ£®æ—ã€Bagging (å¯å¹¶è¡Œ)',
            'è®¡ç®—èµ„æºæœ‰é™': 'AdaBoostã€GBDT (ä¸²è¡Œ)',
            'éœ€è¦åœ¨çº¿å­¦ä¹ ': 'å¢é‡å­¦ä¹ çš„Boostingå˜ä½“'
        },
        'æ¨¡å‹è¦æ±‚': {
            'éœ€è¦ç‰¹å¾é‡è¦æ€§': 'éšæœºæ£®æ— (ç¨³å®š) æˆ– GBDT (ç²¾ç¡®)',
            'éœ€è¦æ¦‚ç‡ä¼°è®¡': 'æ”¯æŒpredict_probaçš„æ¨¡å‹',
            'éœ€è¦å¤„ç†ç¼ºå¤±å€¼': 'éšæœºæ£®æ— (è‡ªåŠ¨å¤„ç†ç¼ºå¤±å€¼)',
            'éœ€è¦å¹¶è¡Œè®­ç»ƒ': 'Baggingã€éšæœºæ£®æ—'
        }
    }

    # æ‰“å°æŒ‡å—
    for section, content in guide.items():
        print(f"\n{section}:")
        for subsection, advice in content.items():
            if isinstance(advice, dict):
                print(f"  {subsection}:")
                for condition, recommendation in advice.items():
                    print(f"    â€¢ {condition}: {recommendation}")
            else:
                print(f"  â€¢ {subsection}: {advice}")

    return guide

if __name__ == "__main__":
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs('../results/figures', exist_ok=True)

    print("å¼€å§‹é›†æˆæ–¹æ³•ç»¼åˆå¯¹æ¯”å®éªŒ...")
    print("-"*60)

    try:
        # 1. å¯¹æ¯”åˆ†ç±»æ–¹æ³•
        print("\n" + "="*60)
        print("ç¬¬ä¸€éƒ¨åˆ†ï¼šåˆ†ç±»æ–¹æ³•å¯¹æ¯”")
        print("="*60)
        df_class_results, models = compare_classification_methods()

        # 2. å¯¹æ¯”å›å½’æ–¹æ³•
        print("\n" + "="*60)
        print("ç¬¬äºŒéƒ¨åˆ†ï¼šå›å½’æ–¹æ³•å¯¹æ¯”")
        print("="*60)
        df_reg_results = compare_regression_methods()

        # 3. åˆ›å»ºé€‰æ‹©æŒ‡å—
        guide = create_selection_guide()

        # 4. ä¿å­˜ç»“æœ
        df_class_results.to_csv('../results/day3_classification_comparison.csv', index=False)
        df_reg_results.to_csv('../results/day3_regression_comparison.csv', index=False)

        # 5. ç»¼åˆåˆ†æ
        print("\n" + "="*60)
        print("ç»¼åˆåˆ†ææŠ¥å‘Š")
        print("="*60)

        print("\nğŸ“Š åˆ†ç±»ä»»åŠ¡:")
        print("-"*40)
        best_class_model = df_class_results.loc[df_class_results['Accuracy'].idxmax(), 'Model']
        best_class_acc = df_class_results['Accuracy'].max()
        print(f"æœ€ä½³æ¨¡å‹: {best_class_model} (å‡†ç¡®ç‡: {best_class_acc:.4f})")

        print("\nğŸ“ˆ å›å½’ä»»åŠ¡:")
        print("-"*40)
        best_reg_model = df_reg_results.loc[df_reg_results['RÂ²'].idxmax(), 'Model']
        best_reg_r2 = df_reg_results['RÂ²'].max()
        best_reg_mse = df_reg_results.loc[df_reg_results['MSE'].idxmin(), 'MSE']
        print(f"æœ€ä½³æ¨¡å‹: {best_reg_model} (RÂ²: {best_reg_r2:.4f}, MSE: {best_reg_mse:.4f})")

        print("\nâ±ï¸ è®­ç»ƒæ•ˆç‡:")
        print("-"*40)
        fastest_class = df_class_results.loc[df_class_results['Train_Time'].idxmin()]
        fastest_reg = df_reg_results.loc[df_reg_results['Train_Time'].idxmin()]
        print(f"æœ€å¿«åˆ†ç±»æ¨¡å‹: {fastest_class['Model']} ({fastest_class['Train_Time']:.3f}s)")
        print(f"æœ€å¿«å›å½’æ¨¡å‹: {fastest_reg['Model']} ({fastest_reg['Train_Time']:.3f}s)")

        print("\nğŸ” å…³é”®å‘ç°:")
        print("-"*40)
        print("1. Baggingæ–¹æ³•é€šå¸¸è®­ç»ƒæœ€å¿«ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰")
        print("2. Boostingæ–¹æ³•é€šå¸¸ç²¾åº¦æœ€é«˜ä½†è®­ç»ƒè¾ƒæ…¢")
        print("3. éšæœºæ£®æ—æ˜¯å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦çš„å¥½é€‰æ‹©")
        print("4. æˆ‘ä»¬çš„GBDTå®ç°æ¥è¿‘sklearnæ€§èƒ½")

        print("\nğŸ¯ å®è·µå»ºè®®:")
        print("-"*40)
        print("â€¢ è¿½æ±‚æœ€é«˜ç²¾åº¦: é€‰æ‹©GBDTï¼Œä»”ç»†è°ƒå‚")
        print("â€¢ éœ€è¦ç¨³å®šæ€§å’Œé€Ÿåº¦: é€‰æ‹©éšæœºæ£®æ—")
        print("â€¢ å¤„ç†ç±»åˆ«ä¸å¹³è¡¡: é€‰æ‹©AdaBoost")
        print("â€¢ å¤§è§„æ¨¡æ•°æ®: é€‰æ‹©Baggingå¹¶è¡Œè®­ç»ƒ")

        print("\n" + "="*60)
        print("å®éªŒå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ° ../results/ ç›®å½•")
        print("="*60)

    except Exception as e:
        print(f"å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()