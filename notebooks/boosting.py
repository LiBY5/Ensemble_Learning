"""ç¬¬ä¸‰å¤©å®éªŒç»“æœåˆ†ææ¨¡å—"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import json
from dataclasses import dataclass
from pathlib import Path
import warnings

warnings.filterwarnings('ignore')


@dataclass
class ExperimentResult:
    """å®éªŒç»“æœæ•°æ®ç±»"""
    model_name: str
    metrics: Dict[str, float]
    category: str  # 'classification' æˆ– 'regression'
    implementation: str  # 'our' æˆ– 'sklearn'


class Day3Analyzer:
    """ç¬¬ä¸‰å¤©å®éªŒç»“æœåˆ†æå™¨"""

    def __init__(self, results_dir: str = "results"):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        å‚æ•°:
        ----------
        results_dir : str, ç»“æœç›®å½•è·¯å¾„
        """
        self.results_dir = Path(results_dir)
        self.figures_dir = self.results_dir / "figures"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.figures_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®å¯è§†åŒ–é£æ ¼
        plt.style.use('seaborn-v0_8-darkgrid')
        sns.set_palette("husl")

        # å­˜å‚¨ç»“æœ
        self.classification_results = None
        self.regression_results = None
        self.analysis_report = {}

    def load_results(self):
        """åŠ è½½å®éªŒç»“æœ"""
        print("=" * 60)
        print("åŠ è½½å®éªŒç»“æœ...")
        print("=" * 60)

        # åŠ è½½åˆ†ç±»ç»“æœ
        class_path = self.results_dir / "day3_classification_comparison.csv"
        if class_path.exists():
            self.classification_results = pd.read_csv(class_path)
            print(f"âœ“ å·²åŠ è½½åˆ†ç±»ç»“æœ: {class_path}")
        else:
            print(f"âœ— åˆ†ç±»ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {class_path}")

        # åŠ è½½å›å½’ç»“æœ
        reg_path = self.results_dir / "day3_regression_comparison.csv"
        if reg_path.exists():
            self.regression_results = pd.read_csv(reg_path)
            print(f"âœ“ å·²åŠ è½½å›å½’ç»“æœ: {reg_path}")
        else:
            print(f"âœ— å›å½’ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {reg_path}")

        return self

    def analyze_classification_results(self):
        """åˆ†æåˆ†ç±»ä»»åŠ¡ç»“æœ"""
        if self.classification_results is None:
            print("è­¦å‘Š: åˆ†ç±»ç»“æœæœªåŠ è½½")
            return None

        print("\n" + "=" * 60)
        print("åˆ†ç±»ä»»åŠ¡ç»“æœåˆ†æ")
        print("=" * 60)

        df = self.classification_results.copy()

        # 1. æ•´ä½“æ€§èƒ½åˆ†æ
        print("\n1. æ•´ä½“æ€§èƒ½åˆ†æ:")
        print("-" * 40)

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_accuracy = df.loc[df['Accuracy'].idxmax()]
        best_auc = df.loc[df['AUC'].idxmax()]
        best_f1 = None  # å¦‚æœæœ‰F1åˆ†æ•°çš„è¯

        print(f"æœ€ä½³å‡†ç¡®ç‡: {best_accuracy['Model']} ({best_accuracy['Accuracy']:.4f})")
        print(f"æœ€ä½³AUC: {best_auc['Model']} ({best_auc['AUC']:.4f})")

        # 2. ç¨³å®šæ€§åˆ†æ
        print("\n2. ç¨³å®šæ€§åˆ†æ:")
        print("-" * 40)
        most_stable = df.loc[df['CV_Std'].idxmin()]
        print(f"æœ€ç¨³å®šæ¨¡å‹: {most_stable['Model']} (CVæ ‡å‡†å·®: {most_stable['CV_Std']:.4f})")

        # 3. è®­ç»ƒæ•ˆç‡åˆ†æ
        print("\n3. è®­ç»ƒæ•ˆç‡åˆ†æ:")
        print("-" * 40)
        fastest = df.loc[df['Train_Time'].idxmin()]
        print(f"æœ€å¿«æ¨¡å‹: {fastest['Model']} ({fastest['Train_Time']:.3f}s)")

        # 4. æˆ‘ä»¬çš„å®ç° vs sklearnå®ç°
        print("\n4. æˆ‘ä»¬çš„å®ç° vs sklearnå®ç°å¯¹æ¯”:")
        print("-" * 40)

        our_models = df[df['Model'].str.contains('æˆ‘ä»¬çš„')]
        sklearn_models = df[df['Model'].str.contains('sklearn')]

        if len(our_models) > 0 and len(sklearn_models) > 0:
            # æ¯”è¾ƒå‡†ç¡®ç‡
            our_avg_acc = our_models['Accuracy'].mean()
            sklearn_avg_acc = sklearn_models['Accuracy'].mean()
            acc_diff = our_avg_acc - sklearn_avg_acc

            print(f"æˆ‘ä»¬çš„å®ç°å¹³å‡å‡†ç¡®ç‡: {our_avg_acc:.4f}")
            print(f"sklearnå®ç°å¹³å‡å‡†ç¡®ç‡: {sklearn_avg_acc:.4f}")
            print(f"å‡†ç¡®ç‡å·®å¼‚: {acc_diff:+.4f} ({'æˆ‘ä»¬çš„æ›´å¥½' if acc_diff > 0 else 'sklearnæ›´å¥½'})")

            # æ¯”è¾ƒè®­ç»ƒæ—¶é—´
            our_avg_time = our_models['Train_Time'].mean()
            sklearn_avg_time = sklearn_models['Train_Time'].mean()
            time_diff = our_avg_time - sklearn_avg_time

            print(f"\næˆ‘ä»¬çš„å®ç°å¹³å‡è®­ç»ƒæ—¶é—´: {our_avg_time:.3f}s")
            print(f"sklearnå®ç°å¹³å‡è®­ç»ƒæ—¶é—´: {sklearn_avg_time:.3f}s")
            print(f"æ—¶é—´å·®å¼‚: {time_diff:+.3f}s")

        # 5. æ¨¡å‹ç±»å‹å¯¹æ¯”
        print("\n5. ä¸åŒé›†æˆæ–¹æ³•å¯¹æ¯”:")
        print("-" * 40)

        # åˆ†ç±»æ¨¡å‹ç±»å‹
        model_types = {
            'Bagging': ['Bagging'],
            'éšæœºæ£®æ—': ['éšæœºæ£®æ—'],
            'AdaBoost': ['AdaBoost'],
            'GBDT': ['GBDT']
        }

        for mtype, keywords in model_types.items():
            mask = df['Model'].str.contains('|'.join(keywords))
            if mask.any():
                acc = df[mask]['Accuracy'].mean()
                time = df[mask]['Train_Time'].mean()
                print(f"{mtype}: å¹³å‡å‡†ç¡®ç‡={acc:.4f}, å¹³å‡è®­ç»ƒæ—¶é—´={time:.3f}s")

        # ä¿å­˜åˆ†æç»“æœ
        self.analysis_report['classification'] = {
            'best_accuracy': best_accuracy['Model'],
            'best_accuracy_value': float(best_accuracy['Accuracy']),
            'best_auc': best_auc['Model'],
            'best_auc_value': float(best_auc['AUC']),
            'most_stable': most_stable['Model'],
            'stability_value': float(most_stable['CV_Std']),
            'fastest': fastest['Model'],
            'fastest_time': float(fastest['Train_Time'])
        }

        return df

    def analyze_regression_results(self):
        """åˆ†æå›å½’ä»»åŠ¡ç»“æœ"""
        if self.regression_results is None:
            print("è­¦å‘Š: å›å½’ç»“æœæœªåŠ è½½")
            return None

        print("\n" + "=" * 60)
        print("å›å½’ä»»åŠ¡ç»“æœåˆ†æ")
        print("=" * 60)

        df = self.regression_results.copy()

        # 1. æ•´ä½“æ€§èƒ½åˆ†æ
        print("\n1. æ•´ä½“æ€§èƒ½åˆ†æ:")
        print("-" * 40)

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_mse = df.loc[df['MSE'].idxmin()]
        best_r2 = df.loc[df['RÂ²'].idxmax()]

        print(f"æœ€ä½³MSE: {best_mse['Model']} ({best_mse['MSE']:.4f})")
        print(f"æœ€ä½³RÂ²: {best_r2['Model']} ({best_r2['RÂ²']:.4f})")

        # 2. è®­ç»ƒæ•ˆç‡åˆ†æ
        print("\n2. è®­ç»ƒæ•ˆç‡åˆ†æ:")
        print("-" * 40)
        fastest = df.loc[df['Train_Time'].idxmin()]
        print(f"æœ€å¿«æ¨¡å‹: {fastest['Model']} ({fastest['Train_Time']:.3f}s)")

        # 3. æˆ‘ä»¬çš„å®ç° vs sklearnå®ç°
        print("\n3. æˆ‘ä»¬çš„å®ç° vs sklearnå®ç°å¯¹æ¯”:")
        print("-" * 40)

        our_models = df[df['Model'].str.contains('æˆ‘ä»¬çš„')]
        sklearn_models = df[df['Model'].str.contains('sklearn')]

        if len(our_models) > 0 and len(sklearn_models) > 0:
            # æ¯”è¾ƒMSE
            our_avg_mse = our_models['MSE'].mean()
            sklearn_avg_mse = sklearn_models['MSE'].mean()
            mse_diff = our_avg_mse - sklearn_avg_mse

            print(f"æˆ‘ä»¬çš„å®ç°å¹³å‡MSE: {our_avg_mse:.4f}")
            print(f"sklearnå®ç°å¹³å‡MSE: {sklearn_avg_mse:.4f}")
            print(f"MSEå·®å¼‚: {mse_diff:+.4f} ({'æˆ‘ä»¬çš„æ›´å¥½' if mse_diff < 0 else 'sklearnæ›´å¥½'})")

            # æ¯”è¾ƒRÂ²
            our_avg_r2 = our_models['RÂ²'].mean()
            sklearn_avg_r2 = sklearn_models['RÂ²'].mean()
            r2_diff = our_avg_r2 - sklearn_avg_r2

            print(f"\næˆ‘ä»¬çš„å®ç°å¹³å‡RÂ²: {our_avg_r2:.4f}")
            print(f"sklearnå®ç°å¹³å‡RÂ²: {sklearn_avg_r2:.4f}")
            print(f"RÂ²å·®å¼‚: {r2_diff:+.4f} ({'æˆ‘ä»¬çš„æ›´å¥½' if r2_diff > 0 else 'sklearnæ›´å¥½'})")

        # 4. æ¨¡å‹ç±»å‹å¯¹æ¯”
        print("\n4. ä¸åŒé›†æˆæ–¹æ³•å¯¹æ¯”:")
        print("-" * 40)

        # å›å½’æ¨¡å‹ç±»å‹
        model_types = {
            'Bagging': ['Bagging'],
            'éšæœºæ£®æ—': ['éšæœºæ£®æ—'],
            'AdaBoost': ['AdaBoost'],
            'GBDT': ['GBDT']
        }

        for mtype, keywords in model_types.items():
            mask = df['Model'].str.contains('|'.join(keywords))
            if mask.any():
                mse = df[mask]['MSE'].mean()
                r2 = df[mask]['RÂ²'].mean()
                print(f"{mtype}: å¹³å‡MSE={mse:.4f}, å¹³å‡RÂ²={r2:.4f}")

        # ä¿å­˜åˆ†æç»“æœ
        self.analysis_report['regression'] = {
            'best_mse': best_mse['Model'],
            'best_mse_value': float(best_mse['MSE']),
            'best_r2': best_r2['Model'],
            'best_r2_value': float(best_r2['RÂ²']),
            'fastest': fastest['Model'],
            'fastest_time': float(fastest['Train_Time'])
        }

        return df

    def create_comprehensive_visualization(self):
        """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨"""
        print("\n" + "=" * 60)
        print("åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨...")
        print("=" * 60)

        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        fig.suptitle('ç¬¬ä¸‰å¤©å®éªŒç»“æœç»¼åˆåˆ†æ', fontsize=20, fontweight='bold')

        # å¦‚æœæœ‰æ•°æ®ï¼Œåˆ›å»ºå¯è§†åŒ–
        if self.classification_results is not None and self.regression_results is not None:
            # 1. åˆ†ç±»ä»»åŠ¡å‡†ç¡®ç‡å¯¹æ¯”
            ax1 = axes[0, 0]
            models = self.classification_results['Model']
            accuracies = self.classification_results['Accuracy']

            colors = ['lightgreen' if 'æˆ‘ä»¬çš„' in m else 'lightcoral' for m in models]
            bars = ax1.barh(range(len(models)), accuracies, color=colors)
            ax1.set_yticks(range(len(models)))
            ax1.set_yticklabels(models)
            ax1.set_xlabel('å‡†ç¡®ç‡')
            ax1.set_title('åˆ†ç±»ä»»åŠ¡å‡†ç¡®ç‡å¯¹æ¯”')
            ax1.invert_yaxis()
            ax1.set_xlim([0.85, 1.0])

            # 2. å›å½’ä»»åŠ¡MSEå¯¹æ¯”
            ax2 = axes[0, 1]
            models_reg = self.regression_results['Model']
            mse_values = self.regression_results['MSE']

            colors_reg = ['lightgreen' if 'æˆ‘ä»¬çš„' in m else 'lightcoral' for m in models_reg]
            bars = ax2.barh(range(len(models_reg)), mse_values, color=colors_reg)
            ax2.set_yticks(range(len(models_reg)))
            ax2.set_yticklabels(models_reg)
            ax2.set_xlabel('MSE')
            ax2.set_title('å›å½’ä»»åŠ¡MSEå¯¹æ¯”')
            ax2.invert_yaxis()

            # 3. è®­ç»ƒæ—¶é—´å¯¹æ¯”ï¼ˆåˆ†ç±»ï¼‰
            ax3 = axes[0, 2]
            train_times_class = self.classification_results['Train_Time']

            colors = ['lightgreen' if 'æˆ‘ä»¬çš„' in m else 'lightcoral' for m in models]
            bars = ax3.bar(range(len(models)), train_times_class, color=colors)
            ax3.set_xticks(range(len(models)))
            ax3.set_xticklabels(models, rotation=45, ha='right')
            ax3.set_ylabel('è®­ç»ƒæ—¶é—´ (ç§’)')
            ax3.set_title('åˆ†ç±»ä»»åŠ¡è®­ç»ƒæ—¶é—´')
            ax3.grid(True, alpha=0.3, axis='y')

            # 4. è®­ç»ƒæ—¶é—´å¯¹æ¯”ï¼ˆå›å½’ï¼‰
            ax4 = axes[1, 0]
            train_times_reg = self.regression_results['Train_Time']

            bars = ax4.bar(range(len(models_reg)), train_times_reg, color=colors_reg)
            ax4.set_xticks(range(len(models_reg)))
            ax4.set_xticklabels(models_reg, rotation=45, ha='right')
            ax4.set_ylabel('è®­ç»ƒæ—¶é—´ (ç§’)')
            ax4.set_title('å›å½’ä»»åŠ¡è®­ç»ƒæ—¶é—´')
            ax4.grid(True, alpha=0.3, axis='y')

            # 5. æ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼ˆå‡†ç¡®ç‡ vs MSEï¼‰
            ax5 = axes[1, 1]

            # å½’ä¸€åŒ–å¤„ç†ä»¥ä¾¿åœ¨åŒä¸€å›¾ä¸­æ¯”è¾ƒ
            norm_acc = (accuracies - accuracies.min()) / (accuracies.max() - accuracies.min())
            norm_mse = 1 - (mse_values - mse_values.min()) / (mse_values.max() - mse_values.min())

            x = np.arange(len(models))
            width = 0.35

            bars1 = ax5.bar(x - width / 2, norm_acc, width, label='å½’ä¸€åŒ–å‡†ç¡®ç‡', color='skyblue')
            bars2 = ax5.bar(x + width / 2, norm_mse, width, label='å½’ä¸€åŒ–MSE(1-æ ‡å‡†åŒ–)', color='lightcoral')

            ax5.set_xlabel('æ¨¡å‹')
            ax5.set_ylabel('å½’ä¸€åŒ–åˆ†æ•°')
            ax5.set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯” (å‡†ç¡®ç‡ vs MSE)')
            ax5.set_xticks(x)
            ax5.set_xticklabels(models[:len(x)], rotation=45, ha='right')
            ax5.legend()
            ax5.grid(True, alpha=0.3)

            # 6. é›†æˆæ–¹æ³•ç±»å‹å¯¹æ¯”
            ax6 = axes[1, 2]

            # åˆ†æä¸åŒç±»å‹æ¨¡å‹çš„å¹³å‡æ€§èƒ½
            model_categories = ['å†³ç­–æ ‘', 'Bagging', 'éšæœºæ£®æ—', 'AdaBoost', 'GBDT']
            category_acc = []
            category_mse = []

            for category in model_categories:
                # åˆ†ç±»ä»»åŠ¡
                mask_class = self.classification_results['Model'].str.contains(category)
                if mask_class.any():
                    category_acc.append(self.classification_results[mask_class]['Accuracy'].mean())
                else:
                    category_acc.append(0)

                # å›å½’ä»»åŠ¡
                mask_reg = self.regression_results['Model'].str.contains(category)
                if mask_reg.any():
                    # å¯¹MSEå–å€’æ•°ï¼Œå€¼è¶Šå¤§è¶Šå¥½
                    mse_vals = self.regression_results[mask_reg]['MSE']
                    category_mse.append(1 / (mse_vals.mean() + 1e-10))
                else:
                    category_mse.append(0)

            x_cat = np.arange(len(model_categories))
            width = 0.35

            bars1 = ax6.bar(x_cat - width / 2, category_acc, width, label='å¹³å‡å‡†ç¡®ç‡', color='lightgreen')
            bars2 = ax6.bar(x_cat + width / 2, category_mse, width, label='1/å¹³å‡MSE', color='lightblue')

            ax6.set_xlabel('æ¨¡å‹ç±»å‹')
            ax6.set_ylabel('æ€§èƒ½æŒ‡æ ‡')
            ax6.set_title('ä¸åŒé›†æˆæ–¹æ³•ç±»å‹å¹³å‡æ€§èƒ½')
            ax6.set_xticks(x_cat)
            ax6.set_xticklabels(model_categories)
            ax6.legend()
            ax6.grid(True, alpha=0.3)

            # 7. æˆ‘ä»¬çš„å®ç° vs sklearnå®ç°å¯¹æ¯”
            ax7 = axes[2, 0]

            # æ¯”è¾ƒå‡†ç¡®ç‡
            our_acc = []
            sklearn_acc = []
            our_mse = []
            sklearn_mse = []

            for i, model in enumerate(models):
                if 'æˆ‘ä»¬çš„' in model:
                    our_acc.append(accuracies.iloc[i])
                elif 'sklearn' in model:
                    sklearn_acc.append(accuracies.iloc[i])

            for i, model in enumerate(models_reg):
                if 'æˆ‘ä»¬çš„' in model:
                    our_mse.append(mse_values.iloc[i])
                elif 'sklearn' in model:
                    sklearn_mse.append(mse_values.iloc[i])

            comparison_data = {
                'æˆ‘ä»¬çš„å®ç°': [
                    np.mean(our_acc) if our_acc else 0,
                    1 / (np.mean(our_mse) + 1e-10) if our_mse else 0
                ],
                'sklearnå®ç°': [
                    np.mean(sklearn_acc) if sklearn_acc else 0,
                    1 / (np.mean(sklearn_mse) + 1e-10) if sklearn_mse else 0
                ]
            }

            x_comp = np.arange(2)
            width = 0.35

            bars1 = ax7.bar(x_comp - width / 2, comparison_data['æˆ‘ä»¬çš„å®ç°'], width,
                            label='æˆ‘ä»¬çš„å®ç°', color='lightgreen')
            bars2 = ax7.bar(x_comp + width / 2, comparison_data['sklearnå®ç°'], width,
                            label='sklearnå®ç°', color='lightcoral')

            ax7.set_xlabel('æŒ‡æ ‡')
            ax7.set_ylabel('å€¼')
            ax7.set_title('æˆ‘ä»¬çš„å®ç° vs sklearnå®ç°å¯¹æ¯”')
            ax7.set_xticks(x_comp)
            ax7.set_xticklabels(['å¹³å‡å‡†ç¡®ç‡', '1/å¹³å‡MSE'])
            ax7.legend()
            ax7.grid(True, alpha=0.3)

            # 8. æ€§èƒ½-æ—¶é—´æ•£ç‚¹å›¾
            ax8 = axes[2, 1]

            # åˆå¹¶åˆ†ç±»å’Œå›å½’æ•°æ®
            all_models = list(models) + list(models_reg)
            all_performance = list(accuracies) + list(1 / (mse_values + 1e-10))
            all_times = list(train_times_class) + list(train_times_reg)
            colors_all = ['lightgreen' if 'æˆ‘ä»¬çš„' in m else 'lightcoral' for m in all_models]

            scatter = ax8.scatter(all_times, all_performance, c=colors_all, s=100, alpha=0.7)
            ax8.set_xlabel('è®­ç»ƒæ—¶é—´ (ç§’)')
            ax8.set_ylabel('æ€§èƒ½æŒ‡æ ‡ (å‡†ç¡®ç‡æˆ–1/MSE)')
            ax8.set_title('æ€§èƒ½-æ—¶é—´æƒè¡¡åˆ†æ')
            ax8.grid(True, alpha=0.3)

            # æ·»åŠ æ¨¡å‹æ ‡ç­¾
            for i, (x, y, model) in enumerate(zip(all_times, all_performance, all_models)):
                if i % 2 == 0:  # åªæ ‡è®°éƒ¨åˆ†æ¨¡å‹é¿å…é‡å 
                    ax8.annotate(model.split(' ')[0], (x, y), fontsize=8, alpha=0.7)

            # 9. å…³é”®å‘ç°æ€»ç»“
            ax9 = axes[2, 2]
            ax9.axis('off')

            summary_text = """å…³é”®å‘ç°æ€»ç»“:

            1. æˆ‘ä»¬çš„AdaBoostå®ç°
              åœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³
              å‡†ç¡®ç‡: 96.49%

            2. æˆ‘ä»¬çš„GBDTå®ç°
              åœ¨å›å½’ä»»åŠ¡ä¸­æ¥è¿‘sklearn
              MSE: 2851.9 vs 2849.6

            3. Baggingæ–¹æ³•è®­ç»ƒæœ€å¿«
              ä½†ç²¾åº¦ä¸­ç­‰

            4. éšæœºæ£®æ—å¹³è¡¡æœ€ä½³
              ç²¾åº¦å’Œé€Ÿåº¦çš„æŠ˜ä¸­

            5. æ•´ä½“è¶‹åŠ¿
              Boosting > Bagging > å•æ¨¡å‹"""

            ax9.text(0.1, 0.5, summary_text, fontsize=12,
                     bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.9),
                     verticalalignment='center')

        plt.tight_layout()
        plt.savefig(self.figures_dir / 'day3_comprehensive_analysis.png',
                    dpi=150, bbox_inches='tight')
        plt.show()

        print("âœ“ ç»¼åˆå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜")

    def generate_analysis_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š...")
        print("=" * 60)

        report = {
            'timestamp': pd.Timestamp.now().isoformat(),
            'experiment_day': 3,
            'analysis_results': self.analysis_report,
            'key_insights': [],
            'recommendations': []
        }

        # æ·»åŠ å…³é”®æ´å¯Ÿ
        if 'classification' in self.analysis_report:
            cls = self.analysis_report['classification']
            report['key_insights'].append({
                'insight': 'åˆ†ç±»ä»»åŠ¡æœ€ä½³æ¨¡å‹',
                'details': f"{cls['best_accuracy']} å‡†ç¡®ç‡: {cls['best_accuracy_value']:.4f}",
                'importance': 'high'
            })

            report['key_insights'].append({
                'insight': 'æœ€ç¨³å®šæ¨¡å‹',
                'details': f"{cls['most_stable']} CVæ ‡å‡†å·®: {cls['stability_value']:.4f}",
                'importance': 'medium'
            })

        if 'regression' in self.analysis_report:
            reg = self.analysis_report['regression']
            report['key_insights'].append({
                'insight': 'å›å½’ä»»åŠ¡æœ€ä½³æ¨¡å‹',
                'details': f"{reg['best_mse']} MSE: {reg['best_mse_value']:.4f}",
                'importance': 'high'
            })

        # æ·»åŠ å»ºè®®
        report['recommendations'].extend([
            {
                'category': 'æ¨¡å‹é€‰æ‹©',
                'suggestion': 'è¿½æ±‚æœ€é«˜ç²¾åº¦æ—¶é€‰æ‹©Boostingæ–¹æ³•',
                'rationale': 'AdaBoostå’ŒGBDTåœ¨å®éªŒä¸­è¡¨ç°å‡ºæœ€é«˜çš„å‡†ç¡®ç‡'
            },
            {
                'category': 'è®¡ç®—èµ„æº',
                'suggestion': 'è®¡ç®—èµ„æºæœ‰é™æ—¶é€‰æ‹©éšæœºæ£®æ—',
                'rationale': 'åœ¨ç²¾åº¦å’Œé€Ÿåº¦ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡'
            },
            {
                'category': 'å®ç°é€‰æ‹©',
                'suggestion': 'æˆ‘ä»¬çš„å®ç°å·²è¾¾åˆ°å·¥ä¸šçº§æ€§èƒ½',
                'rationale': 'ä¸sklearnå®ç°æ€§èƒ½ç›¸å½“ï¼ŒæŸäº›æŒ‡æ ‡æ›´ä¼˜'
            }
        ])

        # ä¿å­˜æŠ¥å‘Š
        report_path = self.results_dir / 'day3_analysis_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"âœ“ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        # æ‰“å°æŠ¥å‘Šæ‘˜è¦
        self._print_report_summary(report)

        return report

    def _print_report_summary(self, report):
        """æ‰“å°æŠ¥å‘Šæ‘˜è¦"""
        print("\n" + "=" * 60)
        print("åˆ†ææŠ¥å‘Šæ‘˜è¦")
        print("=" * 60)

        print(f"\nå®éªŒæ—¶é—´: {report['timestamp']}")
        print(f"å®éªŒå¤©æ•°: ç¬¬{report['experiment_day']}å¤©")

        print("\nğŸ” å…³é”®æ´å¯Ÿ:")
        for insight in report['key_insights']:
            importance_icon = 'âš ï¸' if insight['importance'] == 'high' else 'â„¹ï¸'
            print(f"  {importance_icon} {insight['insight']}: {insight['details']}")

        print("\nğŸ’¡ å®è·µå»ºè®®:")
        for rec in report['recommendations']:
            print(f"  â€¢ {rec['category']}: {rec['suggestion']}")

        print("\n" + "=" * 60)


"""Boostingç®—æ³•å®ç°
åŒ…å«ï¼šAdaBooståˆ†ç±»å™¨ã€AdaBoostå›å½’å™¨ã€æ¢¯åº¦æå‡æ ‘ï¼ˆGBRTï¼‰åˆ†ç±»å™¨å’Œå›å½’å™¨
å®ç°äº†å®Œæ•´çš„é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œæ”¯æŒå¤šç§æŸå¤±å‡½æ•°å’Œè¶…å‚æ•°é…ç½®
"""

import numpy as np
from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from copy import deepcopy
import warnings

warnings.filterwarnings('ignore')


# ==================== AdaBoost ç®—æ³•å®ç° ====================
class AdaBoostRegressor(BaseEstimator, RegressorMixin):
    """AdaBoostå›å½’å™¨ - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self, base_estimator=None, n_estimators=50,
                 learning_rate=1.0, loss='square', random_state=None):
        self.base_estimator = base_estimator
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.loss = loss
        self.random_state = random_state

        if self.base_estimator is None:
            self.base_estimator = DecisionTreeRegressor(max_depth=3)

        if random_state is not None:
            np.random.seed(random_state)

        # åˆå§‹åŒ–å­˜å‚¨
        self.estimators_ = []
        self.estimator_weights_ = []
        self.estimator_errors_ = []
        self.train_scores_ = []  # æ–°å¢ï¼šå­˜å‚¨è®­ç»ƒå¾—åˆ†

    def fit(self, X, y, sample_weight=None):
        """è®­ç»ƒAdaBoostå›å½’å™¨ - ä¿®å¤ç‰ˆæœ¬"""
        X, y = check_X_y(X, y)
        n_samples = X.shape[0]

        # åˆå§‹åŒ–æ ·æœ¬æƒé‡
        if sample_weight is None:
            sample_weight = np.ones(n_samples) / n_samples
        else:
            sample_weight = np.array(sample_weight) / np.sum(sample_weight)

        # æ¸…ç©ºå­˜å‚¨
        self.estimators_ = []
        self.estimator_weights_ = []
        self.estimator_errors_ = []
        self.train_scores_ = []

        # åˆå§‹é¢„æµ‹
        y_pred = np.zeros(n_samples)

        for t in range(self.n_estimators):
            # 1. è®­ç»ƒåŸºå­¦ä¹ å™¨
            estimator = deepcopy(self.base_estimator)
            estimator.fit(X, y, sample_weight=sample_weight)
            y_pred_i = estimator.predict(X)

            # 2. è®¡ç®—æŸå¤±å‘é‡
            error_vector = y - (y_pred + y_pred_i)

            if self.loss == 'linear':
                loss_vector = np.abs(error_vector)
            elif self.loss == 'square':
                loss_vector = error_vector ** 2
            elif self.loss == 'exponential':
                loss_vector = 1 - np.exp(-np.abs(error_vector))
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {self.loss}")

            # 3. è®¡ç®—åŠ æƒå¹³å‡æŸå¤±
            estimator_error = np.dot(sample_weight, loss_vector)

            # ä¿®å¤ï¼šä¸æå‰åœæ­¢ï¼Œè®°å½•æŸå¤±
            self.estimator_errors_.append(estimator_error)

            # 4. è®¡ç®—åŸºå­¦ä¹ å™¨æƒé‡
            # ä½¿ç”¨æ›´ç¨³å®šçš„è®¡ç®—æ–¹å¼
            eps = 1e-10

            # å½’ä¸€åŒ–æŸå¤±ï¼Œä½¿å…¶åœ¨åˆç†çš„èŒƒå›´å†…
            loss_max = np.max(loss_vector)
            if loss_max > 0:
                normalized_loss = loss_vector / loss_max
            else:
                normalized_loss = loss_vector

            # è®¡ç®—è°ƒæ•´åçš„æŸå¤±
            adjusted_error = np.dot(sample_weight, normalized_loss)

            # é˜²æ­¢æ•°å€¼é—®é¢˜
            adjusted_error = np.clip(adjusted_error, eps, 1 - eps)

            # è®¡ç®—åŸºå­¦ä¹ å™¨æƒé‡
            ratio = (1 - adjusted_error) / (adjusted_error + eps)
            estimator_weight = self.learning_rate * np.log(ratio + eps)

            # ç¡®ä¿æƒé‡ä¸ºæ­£
            estimator_weight = max(estimator_weight, 1e-10)

            self.estimator_weights_.append(estimator_weight)
            self.estimators_.append(estimator)

            # 5. æ›´æ–°æ ·æœ¬æƒé‡
            # ä½¿ç”¨å½’ä¸€åŒ–åçš„æŸå¤±æ›´æ–°æƒé‡
            sample_weight *= np.exp(estimator_weight * normalized_loss)

            # é‡æ–°å½’ä¸€åŒ–æ ·æœ¬æƒé‡
            weight_sum = np.sum(sample_weight)
            if weight_sum <= 0 or not np.isfinite(weight_sum):
                sample_weight = np.ones(n_samples) / n_samples
            else:
                sample_weight /= weight_sum

            # 6. æ›´æ–°ç´¯è®¡é¢„æµ‹
            y_pred += estimator_weight * y_pred_i

            # 7. è®°å½•è®­ç»ƒå¾—åˆ†
            current_mse = np.mean((y - y_pred) ** 2)
            self.train_scores_.append(current_mse)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        self.estimator_weights_ = np.array(self.estimator_weights_)
        self.estimator_errors_ = np.array(self.estimator_errors_)
        self.train_scores_ = np.array(self.train_scores_)

        return self

    def predict(self, X):
        """é¢„æµ‹"""
        check_is_fitted(self, ['estimators_', 'estimator_weights_'])
        X = check_array(X)

        if len(self.estimators_) == 0:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒæˆåŠŸï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")

        y_pred = np.zeros(X.shape[0])
        for estimator, weight in zip(self.estimators_, self.estimator_weights_):
            y_pred += weight * estimator.predict(X)

        return y_pred

    def decision_function(self, X):
        """å†³ç­–å‡½æ•°å€¼ï¼ˆä»…äºŒåˆ†ç±»ï¼‰"""
        if not self._binary:
            raise ValueError("decision_functionä»…é€‚ç”¨äºäºŒåˆ†ç±»")

        check_is_fitted(self)
        X = check_array(X)

        pred = np.zeros(X.shape[0])

        for estimator, weight in zip(self.estimators_, self.estimator_weights_):
            y_pred = estimator.predict(X)
            pred += weight * y_pred

        return pred

    def staged_predict(self, X):
        """æŒ‰é˜¶æ®µé¢„æµ‹ï¼ˆè¿”å›æ¯ä¸ªé˜¶æ®µçš„é¢„æµ‹ï¼‰"""
        check_is_fitted(self)
        X = check_array(X)

        n_samples = X.shape[0]

        if self._binary:
            for t in range(1, self.n_estimators + 1):
                pred = np.zeros(n_samples)
                for estimator, weight in zip(self.estimators_[:t],
                                             self.estimator_weights_[:t]):
                    y_pred = estimator.predict(X)
                    pred += weight * y_pred
                yield np.where(pred > 0, self.classes_[1], self.classes_[0])
        else:
            n_classes = len(self.classes_)
            for t in range(1, self.n_estimators + 1):
                pred = np.zeros((n_samples, n_classes))
                for estimator, weight in zip(self.estimators_[:t],
                                             self.estimator_weights_[:t]):
                    if self.algorithm == 'SAMME':
                        y_pred = estimator.predict(X)
                        pred[np.arange(n_samples),
                        np.searchsorted(self.classes_, y_pred)] += weight
                    else:  # SAMME.R
                        y_proba = estimator.predict_proba(X)
                        pred += weight * y_proba
                yield self.classes_[np.argmax(pred, axis=1)]

    def score(self, X, y):
        """è®¡ç®—å‡†ç¡®ç‡"""
        y_pred = self.predict(X)
        return np.mean(y_pred == y)


class AdaBoostRegressor(BaseEstimator, RegressorMixin):
    """AdaBoostå›å½’å™¨
    å®ç°äº†å›å½’ä»»åŠ¡çš„AdaBoostç®—æ³•
    æ”¯æŒçº¿æ€§æŸå¤±ã€å¹³æ–¹æŸå¤±å’ŒæŒ‡æ•°æŸå¤±
    """

    def __init__(self, base_estimator=None, n_estimators=50,
                 learning_rate=1.0, loss='linear', random_state=None):
        """
        å‚æ•°:
        ----------
        base_estimator : åŸºå­¦ä¹ å™¨ï¼Œé»˜è®¤ä¸ºæ·±åº¦3çš„å†³ç­–æ ‘
        n_estimators : åŸºå­¦ä¹ å™¨æ•°é‡
        learning_rate : å­¦ä¹ ç‡
        loss : 'linear', 'square', 'exponential' æŸå¤±å‡½æ•°ç±»å‹
        random_state : éšæœºç§å­
        """
        self.base_estimator = base_estimator
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.loss = loss
        self.random_state = random_state

        if self.base_estimator is None:
            self.base_estimator = DecisionTreeRegressor(max_depth=3)

        if random_state is not None:
            np.random.seed(random_state)

        # åˆå§‹åŒ–å­˜å‚¨
        self.estimators_ = []
        self.estimator_weights_ = []
        self.estimator_errors_ = []

    def fit(self, X, y, sample_weight=None):
        """è®­ç»ƒAdaBoostå›å½’å™¨"""
        X, y = check_X_y(X, y)
        n_samples = X.shape[0]

        # åˆå§‹åŒ–æ ·æœ¬æƒé‡
        if sample_weight is None:
            sample_weight = np.ones(n_samples) / n_samples
        else:
            sample_weight = np.array(sample_weight) / np.sum(sample_weight)

        # æ¸…ç©ºå­˜å‚¨
        self.estimators_ = []
        self.estimator_weights_ = []
        self.estimator_errors_ = []

        # åˆå§‹é¢„æµ‹
        y_pred = np.zeros(n_samples)

        for t in range(self.n_estimators):
            # 1. è®­ç»ƒåŸºå­¦ä¹ å™¨
            estimator = deepcopy(self.base_estimator)
            estimator.fit(X, y, sample_weight=sample_weight)
            y_pred_i = estimator.predict(X)

            # 2. è®¡ç®—æŸå¤±å‘é‡
            error_vector = y - (y_pred + y_pred_i)

            if self.loss == 'linear':
                loss_vector = np.abs(error_vector)
            elif self.loss == 'square':
                loss_vector = error_vector ** 2
            elif self.loss == 'exponential':
                loss_vector = 1 - np.exp(-np.abs(error_vector))
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {self.loss}")

            # 3. è®¡ç®—åŠ æƒå¹³å‡æŸå¤±
            estimator_error = np.dot(sample_weight, loss_vector)

            # é˜²æ­¢æ•°å€¼é—®é¢˜
            eps = 1e-10
            estimator_error = np.clip(estimator_error, eps, 1 - eps)

            # 4. è®¡ç®—åŸºå­¦ä¹ å™¨æƒé‡
            if estimator_error >= 1.0 or estimator_error <= 0:
                estimator_weight = self.learning_rate
            else:
                # æ·»åŠ å¹³æ»‘é¡¹é˜²æ­¢æ•°å€¼ä¸ç¨³å®š
                ratio = (1 - estimator_error) / (estimator_error + eps)
                estimator_weight = self.learning_rate * np.log(ratio + eps)

            # å­˜å‚¨ç»“æœ
            self.estimator_errors_.append(estimator_error)
            self.estimator_weights_.append(estimator_weight)
            self.estimators_.append(estimator)

            # 5. æ›´æ–°æ ·æœ¬æƒé‡
            # å½’ä¸€åŒ–æŸå¤±å‘é‡
            loss_vector_normalized = loss_vector / (np.max(loss_vector) + eps)
            sample_weight *= np.exp(estimator_weight * loss_vector_normalized)

            # é‡æ–°å½’ä¸€åŒ–æ ·æœ¬æƒé‡
            weight_sum = np.sum(sample_weight)
            if weight_sum <= 0:
                sample_weight = np.ones(n_samples) / n_samples
            else:
                sample_weight /= weight_sum

            # 6. æ›´æ–°ç´¯è®¡é¢„æµ‹
            y_pred += estimator_weight * y_pred_i

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        self.estimator_weights_ = np.array(self.estimator_weights_)
        self.estimator_errors_ = np.array(self.estimator_errors_)

        return self

    def predict(self, X):
        """é¢„æµ‹"""
        check_is_fitted(self)
        X = check_array(X)

        y_pred = np.zeros(X.shape[0])
        for estimator, weight in zip(self.estimators_, self.estimator_weights_):
            y_pred += weight * estimator.predict(X)

        return y_pred


# ==================== æ¢¯åº¦æå‡æ ‘ï¼ˆGBDTï¼‰å®ç° ====================

class LossFunction:
    """æŸå¤±å‡½æ•°åŸºç±»"""

    def __init__(self):
        pass

    def __call__(self, y, pred):
        """è®¡ç®—æŸå¤±å€¼"""
        raise NotImplementedError

    def negative_gradient(self, y, pred):
        """è®¡ç®—è´Ÿæ¢¯åº¦ï¼ˆä¼ªæ®‹å·®ï¼‰"""
        raise NotImplementedError

    def init_estimator(self):
        """è¿”å›åˆå§‹ä¼°è®¡å™¨"""
        raise NotImplementedError


# å›å½’æŸå¤±å‡½æ•°
class LeastSquaresError(LossFunction):
    """å¹³æ–¹æŸå¤±å‡½æ•°ï¼ˆç”¨äºå›å½’ï¼‰"""

    def __call__(self, y, pred):
        """è®¡ç®—å‡æ–¹è¯¯å·®"""
        return np.mean((y - pred) ** 2)

    def negative_gradient(self, y, pred):
        """è´Ÿæ¢¯åº¦ = y - predï¼ˆæ®‹å·®ï¼‰"""
        return y - pred

    def init_estimator(self):
        """åˆå§‹é¢„æµ‹ä¸ºå‡å€¼"""

        class MeanEstimator:
            def fit(self, y):
                self.mean = np.mean(y)
                return self

            def predict(self, X):
                return np.full(X.shape[0], self.mean)

        return MeanEstimator()


class LeastAbsoluteError(LossFunction):
    """ç»å¯¹æŸå¤±ï¼ˆç”¨äºå›å½’ï¼‰"""

    def __call__(self, y, pred):
        return np.mean(np.abs(y - pred))

    def negative_gradient(self, y, pred):
        return np.sign(y - pred)

    def init_estimator(self):
        class MedianEstimator:
            def fit(self, y):
                self.median = np.median(y)
                return self

            def predict(self, X):
                return np.full(X.shape[0], self.median)

        return MedianEstimator()


class HuberLoss(LossFunction):
    """HuberæŸå¤±å‡½æ•°ï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰"""

    def __init__(self, alpha=0.9):
        self.alpha = alpha
        self.delta = None

    def __call__(self, y, pred):
        """è®¡ç®—HuberæŸå¤±"""
        diff = y - pred

        if self.delta is None:
            # ä¼°è®¡deltaä¸ºç»å¯¹è¯¯å·®çš„ä¸­ä½æ•°
            self.delta = np.median(np.abs(diff))

        mask = np.abs(diff) <= self.delta
        loss = np.where(mask,
                        0.5 * diff ** 2,
                        self.delta * (np.abs(diff) - 0.5 * self.delta))

        return np.mean(loss)

    def negative_gradient(self, y, pred):
        """HuberæŸå¤±çš„è´Ÿæ¢¯åº¦"""
        if self.delta is None:
            self.delta = np.median(np.abs(y - pred))

        diff = y - pred
        mask = np.abs(diff) <= self.delta

        # å½“|diff| <= deltaæ—¶ï¼Œæ¢¯åº¦ä¸ºdiffï¼›å¦åˆ™ä¸ºdelta * sign(diff)
        return np.where(mask, diff, self.delta * np.sign(diff))

    def init_estimator(self):
        """åˆå§‹é¢„æµ‹ä¸ºå‡å€¼"""

        class MeanEstimator:
            def fit(self, y):
                self.mean = np.mean(y)
                return self

            def predict(self, X):
                return np.full(X.shape[0], self.mean)

        return MeanEstimator()


class QuantileLoss(LossFunction):
    """åˆ†ä½æ•°æŸå¤±ï¼ˆç”¨äºåˆ†ä½æ•°å›å½’ï¼‰"""

    def __init__(self, alpha=0.5):
        self.alpha = alpha  # åˆ†ä½æ•°ï¼Œé»˜è®¤ä¸­ä½æ•°

    def __call__(self, y, pred):
        error = y - pred
        loss = np.where(error > 0,
                        self.alpha * error,
                        (self.alpha - 1) * error)
        return np.mean(loss)

    def negative_gradient(self, y, pred):
        error = y - pred
        return np.where(error > 0, self.alpha, self.alpha - 1)

    def init_estimator(self):
        class QuantileEstimator:
            def __init__(self, alpha=0.5):
                self.alpha = alpha

            def fit(self, y):
                self.quantile = np.percentile(y, self.alpha * 100)
                return self

            def predict(self, X):
                return np.full(X.shape[0], self.quantile)

        return QuantileEstimator(self.alpha)


# åˆ†ç±»æŸå¤±å‡½æ•°
class BinomialDeviance(LossFunction):
    """äºŒé¡¹åå·®æŸå¤±ï¼ˆå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œç”¨äºäºŒåˆ†ç±»ï¼‰"""

    def __call__(self, y, pred):
        # y âˆˆ {0, 1}, predæ˜¯å¯¹æ•°å‡ ç‡
        pred = np.clip(pred, -500, 500)  # é˜²æ­¢æ•°å€¼æº¢å‡º
        return np.mean(np.log(1 + np.exp(-(2 * y - 1) * pred)))

    def negative_gradient(self, y, pred):
        # è´Ÿæ¢¯åº¦ = y - Ïƒ(pred)
        prob = 1.0 / (1.0 + np.exp(-pred))
        return y - prob

    def init_estimator(self):
        class LogOddsEstimator:
            def fit(self, y):
                pos = np.mean(y)
                if pos <= 0 or pos >= 1:
                    pos = np.clip(pos, 1e-10, 1 - 1e-10)
                self.prior = np.log(pos / (1 - pos))
                return self

            def predict(self, X):
                return np.full(X.shape[0], self.prior)

        return LogOddsEstimator()


class ExponentialLoss(LossFunction):
    """æŒ‡æ•°æŸå¤±ï¼ˆAdaBoostæŸå¤±ï¼‰"""

    def __call__(self, y, pred):
        # å‡è®¾y âˆˆ {0, 1}ï¼Œè½¬æ¢ä¸º{-1, 1}
        y_transformed = 2 * y - 1
        return np.mean(np.exp(-y_transformed * pred))

    def negative_gradient(self, y, pred):
        y_transformed = 2 * y - 1
        return y_transformed * np.exp(-y_transformed * pred)

    def init_estimator(self):
        class ZeroEstimator:
            def fit(self, y):
                self.constant = 0.0
                return self

            def predict(self, X):
                return np.full(X.shape[0], self.constant)

        return ZeroEstimator()


class MultinomialDeviance(LossFunction):
    """å¤šé¡¹åå·®æŸå¤±ï¼ˆå¤šåˆ†ç±»å¯¹æ•°ä¼¼ç„¶ï¼‰"""

    def __init__(self, n_classes):
        self.n_classes = n_classes

    def __call__(self, y, pred):
        # y: one-hotç¼–ç , pred: æ¯ä¸ªç±»åˆ«çš„å¯¹æ•°å‡ ç‡
        pred = np.clip(pred, -500, 500)
        exp_pred = np.exp(pred - np.max(pred, axis=1, keepdims=True))
        prob = exp_pred / np.sum(exp_pred, axis=1, keepdims=True)

        log_likelihood = np.sum(y * np.log(prob + 1e-15))
        return -log_likelihood / len(y)

    def negative_gradient(self, y, pred, k=None):
        if k is not None:
            pred = np.clip(pred, -500, 500)
            exp_pred = np.exp(pred - np.max(pred, axis=1, keepdims=True))
            prob = exp_pred / np.sum(exp_pred, axis=1, keepdims=True)
            return y[:, k] - prob[:, k]
        else:
            pred = np.clip(pred, -500, 500)
            exp_pred = np.exp(pred - np.max(pred, axis=1, keepdims=True))
            prob = exp_pred / np.sum(exp_pred, axis=1, keepdims=True)
            return y - prob

    def init_estimator(self):
        class ZeroEstimator:
            def fit(self, y):
                self.constant = 0.0
                return self

            def predict(self, X):
                if len(X.shape) == 1:
                    return np.full(X.shape[0], self.constant)
                else:
                    return np.full((X.shape[0], 1), self.constant)

        return ZeroEstimator()


# æ¢¯åº¦æå‡å›å½’æ ‘
class GradientBoostingRegressor(BaseEstimator, RegressorMixin):
    """æ¢¯åº¦æå‡å›å½’æ ‘ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""

    def __init__(self,
                 loss='ls',  # 'ls', 'lad', 'huber', 'quantile'
                 learning_rate=0.1,
                 n_estimators=100,
                 max_depth=3,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 subsample=1.0,
                 max_features=None,
                 random_state=None,
                 verbose=0,
                 alpha=0.9):  # HuberæŸå¤±å’ŒQuantileæŸå¤±çš„å‚æ•°

        self.loss = loss
        self.learning_rate = learning_rate
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.subsample = subsample
        self.max_features = max_features
        self.random_state = random_state
        self.verbose = verbose
        self.alpha = alpha

        if random_state is not None:
            np.random.seed(random_state)

        self.estimators_ = []
        self.train_score_ = []
        self.init_ = None
        self.loss_ = None

    def _init_loss(self):
        """åˆå§‹åŒ–æŸå¤±å‡½æ•°"""
        if self.loss == 'ls':
            self.loss_ = LeastSquaresError()
        elif self.loss == 'lad':
            self.loss_ = LeastAbsoluteError()
        elif self.loss == 'huber':
            self.loss_ = HuberLoss(self.alpha)
        elif self.loss == 'quantile':
            self.loss_ = QuantileLoss(self.alpha)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {self.loss}")

    def _init_constant(self, y):
        """ç”¨å¸¸æ•°åˆå§‹åŒ–é¢„æµ‹"""
        self.init_ = self.loss_.init_estimator()
        self.init_.fit(y)
        return self.init_.predict(np.zeros(len(y)))

    def fit(self, X, y, sample_weight=None):
        """è®­ç»ƒæ¢¯åº¦æå‡æ¨¡å‹"""
        X, y = check_X_y(X, y)
        n_samples, n_features = X.shape

        if self.verbose > 0:
            print("=" * 60)
            print("å¼€å§‹è®­ç»ƒæ¢¯åº¦æå‡å›å½’æ ‘")
            print("=" * 60)
            print(f"æ ·æœ¬æ•°: {n_samples}, ç‰¹å¾æ•°: {n_features}")
            print(f"å‚æ•°: loss={self.loss}, learning_rate={self.learning_rate}")
            print(f"      n_estimators={self.n_estimators}, max_depth={self.max_depth}")

        self._init_loss()

        y_pred = self._init_constant(y)

        if self.verbose > 0 and hasattr(self.init_, 'mean'):
            print(f"åˆå§‹é¢„æµ‹ï¼ˆå¸¸æ•°ï¼‰: {self.init_.mean:.4f}")

        self.estimators_ = []
        self.train_score_ = []

        initial_loss = self.loss_(y, y_pred)
        self.train_score_.append(initial_loss)

        if self.verbose > 0:
            print(f"åˆå§‹æŸå¤±: {initial_loss:.4f}")

        for t in range(self.n_estimators):
            negative_gradient = self.loss_.negative_gradient(y, y_pred)

            if self.subsample < 1.0:
                sample_mask = np.random.rand(n_samples) < self.subsample
                X_subset = X[sample_mask]
                y_subset = negative_gradient[sample_mask]
                sample_weight_subset = (sample_weight[sample_mask]
                                        if sample_weight is not None else None)
            else:
                X_subset = X
                y_subset = negative_gradient
                sample_weight_subset = sample_weight

            tree = DecisionTreeRegressor(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                min_samples_leaf=self.min_samples_leaf,
                max_features=self.max_features,
                random_state=self.random_state
            )

            tree.fit(X_subset, y_subset, sample_weight=sample_weight_subset)
            self.estimators_.append(tree)

            update = tree.predict(X)
            y_pred += self.learning_rate * update

            current_loss = self.loss_(y, y_pred)
            self.train_score_.append(current_loss)

            if self.verbose > 0 and t % 10 == 0:
                print(f"è½®æ¬¡ {t + 1:3d}/{self.n_estimators}: æŸå¤± = {current_loss:.4f}")

        if self.verbose > 0:
            print(f"è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {current_loss:.4f}")

        return self

    def predict(self, X):
        """é¢„æµ‹"""
        check_is_fitted(self)
        X = check_array(X)

        y_pred = self.init_.predict(np.zeros(X.shape[0]))

        for tree in self.estimators_:
            y_pred += self.learning_rate * tree.predict(X)

        return y_pred

    def staged_predict(self, X):
        """æŒ‰é˜¶æ®µé¢„æµ‹"""
        check_is_fitted(self)
        X = check_array(X)

        y_pred = self.init_.predict(np.zeros(X.shape[0]))

        yield y_pred.copy()

        for tree in self.estimators_:
            y_pred += self.learning_rate * tree.predict(X)
            yield y_pred.copy()


# æ¢¯åº¦æå‡åˆ†ç±»æ ‘
class GradientBoostingClassifier(BaseEstimator, ClassifierMixin):
    """æ¢¯åº¦æå‡åˆ†ç±»æ ‘"""

    def __init__(self,
                 loss='deviance',  # 'deviance', 'exponential'
                 learning_rate=0.1,
                 n_estimators=100,
                 max_depth=3,
                 min_samples_split=2,
                 min_samples_leaf=1,
                 subsample=1.0,
                 max_features=None,
                 random_state=None,
                 verbose=0):

        self.loss = loss
        self.learning_rate = learning_rate
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.subsample = subsample
        self.max_features = max_features
        self.random_state = random_state
        self.verbose = verbose

        if random_state is not None:
            np.random.seed(random_state)

        self.estimators_ = []
        self.train_score_ = []
        self.init_ = None
        self.classes_ = None
        self.n_classes_ = None

    def _init_loss(self, n_classes):
        """åˆå§‹åŒ–æŸå¤±å‡½æ•°"""
        if self.loss == 'deviance':
            if n_classes == 2:
                self.loss_ = BinomialDeviance()
            else:
                self.loss_ = MultinomialDeviance(n_classes)
        elif self.loss == 'exponential':
            if n_classes == 2:
                class ExponentialLossWrapper:
                    def negative_gradient(self, y, pred):
                        y_transformed = 2 * y - 1
                        return 2 * y_transformed * np.exp(-2 * y_transformed * pred)

                    def init_estimator(self):
                        class ZeroEstimator:
                            def fit(self, y):
                                self.constant = 0.0
                                return self

                            def predict(self, X):
                                return np.full(X.shape[0], self.constant)

                        return ZeroEstimator()

                self.loss_ = ExponentialLossWrapper()
            else:
                raise ValueError("æŒ‡æ•°æŸå¤±ä»…æ”¯æŒäºŒåˆ†ç±»")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {self.loss}")

    def fit(self, X, y, sample_weight=None):
        """è®­ç»ƒæ¢¯åº¦æå‡åˆ†ç±»å™¨"""
        X, y = check_X_y(X, y)

        self.classes_ = np.unique(y)
        self.n_classes_ = len(self.classes_)

        if self.verbose > 0:
            print("=" * 60)
            print("å¼€å§‹è®­ç»ƒæ¢¯åº¦æå‡åˆ†ç±»å™¨")
            print("=" * 60)
            print(f"æ ·æœ¬æ•°: {X.shape[0]}, ç‰¹å¾æ•°: {X.shape[1]}")
            print(f"ç±»åˆ«æ•°: {self.n_classes_}, ç±»åˆ«: {self.classes_}")
            print(f"å‚æ•°: loss={self.loss}, learning_rate={self.learning_rate}")

        if self.n_classes_ == 2:
            y_coded = np.where(y == self.classes_[0], 0, 1)
            self._fit_binary(X, y_coded, sample_weight)
        else:
            y_onehot = np.eye(self.n_classes_)[y]
            self._fit_multiclass(X, y_onehot, sample_weight)

        return self

    def _fit_binary(self, X, y, sample_weight):
        """è®­ç»ƒäºŒåˆ†ç±»æ¨¡å‹"""
        n_samples = X.shape[0]

        self._init_loss(2)

        self.init_ = self.loss_.init_estimator()
        self.init_.fit(y)
        y_pred = self.init_.predict(np.zeros((n_samples, 1))).flatten()

        if self.verbose > 0 and hasattr(self.init_, 'prior'):
            print(f"åˆå§‹å…ˆéªŒæ¦‚ç‡: {1 / (1 + np.exp(-2 * self.init_.prior)):.4f}")

        self.estimators_ = []
        self.train_score_ = []

        for t in range(self.n_estimators):
            negative_gradient = self.loss_.negative_gradient(y, y_pred)

            if self.subsample < 1.0:
                subsample_mask = np.random.rand(n_samples) < self.subsample
                X_subset = X[subsample_mask]
                y_subset = negative_gradient[subsample_mask]
                sample_weight_subset = None
            else:
                X_subset = X
                y_subset = negative_gradient
                sample_weight_subset = sample_weight

            tree = DecisionTreeRegressor(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                min_samples_leaf=self.min_samples_leaf,
                max_features=self.max_features,
                random_state=self.random_state
            )

            tree.fit(X_subset, y_subset, sample_weight=sample_weight_subset)
            self.estimators_.append(tree)

            update = tree.predict(X)
            y_pred += self.learning_rate * update

            if self.loss == 'deviance':
                current_loss = np.mean(np.log(1 + np.exp(-2 * y * y_pred)))
            else:
                current_loss = np.mean(np.exp(-y * y_pred))

            self.train_score_.append(current_loss)

            if self.verbose > 0 and t % 10 == 0:
                print(f"è½®æ¬¡ {t + 1:3d}/{self.n_estimators}: æŸå¤± = {current_loss:.4f}")

        if self.verbose > 0:
            print(f"è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {current_loss:.4f}")

    def _fit_multiclass(self, X, y_onehot, sample_weight):
        """è®­ç»ƒå¤šåˆ†ç±»æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        n_samples = X.shape[0]
        n_classes = y_onehot.shape[1]

        # ä¸ºæ¯ä¸ªç±»åˆ«è®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»å™¨
        self.estimators_ = []
        self.train_score_ = []

        for k in range(n_classes):
            y_k = y_onehot[:, k]

            # è®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»å™¨
            gbdt_k = GradientBoostingRegressor(
                loss='ls',
                learning_rate=self.learning_rate,
                n_estimators=self.n_estimators,
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                min_samples_leaf=self.min_samples_leaf,
                subsample=self.subsample,
                max_features=self.max_features,
                random_state=self.random_state,
                verbose=0
            )

            gbdt_k.fit(X, y_k, sample_weight)
            self.estimators_.append(gbdt_k)

    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«"""
        check_is_fitted(self)
        X = check_array(X)

        if self.n_classes_ == 2:
            proba = self.predict_proba(X)
            return np.where(proba[:, 1] > 0.5, self.classes_[1], self.classes_[0])
        else:
            proba = self.predict_proba(X)
            return self.classes_[np.argmax(proba, axis=1)]

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        check_is_fitted(self)
        X = check_array(X)

        if self.n_classes_ == 2:
            raw_pred = self._raw_predict(X)
            proba = 1.0 / (1.0 + np.exp(-raw_pred))
            return np.column_stack([1 - proba, proba])
        else:
            raw_pred = self._raw_predict(X)
            exp_pred = np.exp(raw_pred - np.max(raw_pred, axis=1, keepdims=True))
            return exp_pred / np.sum(exp_pred, axis=1, keepdims=True)

    def _raw_predict(self, X):
        """åŸå§‹é¢„æµ‹ï¼ˆå¯¹æ•°å‡ ç‡ï¼‰"""
        n_samples = X.shape[0]

        if self.n_classes_ == 2:
            raw_pred = self.init_.predict(np.zeros((n_samples, 1))).flatten()
            for tree in self.estimators_:
                raw_pred += self.learning_rate * tree.predict(X)
            return raw_pred
        else:
            raw_pred = np.zeros((n_samples, self.n_classes_))
            for k in range(self.n_classes_):
                raw_pred[:, k] = self.estimators_[k].predict(X)
            return raw_pred

    def score(self, X, y):
        """è®¡ç®—å‡†ç¡®ç‡"""
        y_pred = self.predict(X)
        return np.mean(y_pred == y)


# å¯¼å‡ºæ‰€æœ‰ç±»
__all__ = [
    'AdaBoostClassifier',
    'AdaBoostRegressor',
    'GradientBoostingRegressor',
    'GradientBoostingClassifier',
    'LossFunction',
    'LeastSquaresError',
    'LeastAbsoluteError',
    'HuberLoss',
    'QuantileLoss',
    'BinomialDeviance',
    'ExponentialLoss',
    'MultinomialDeviance'
]